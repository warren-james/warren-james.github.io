[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Warren James, PhD",
    "section": "",
    "text": "I am a researcher interested in using a whole host of statistical methods to answer all sorts of interesting (not always) questions. The work I do is all in R and so I thought to save me time I could put some of this on a website somewhere that other people might also find useful.\nCurrently, I’m employed as part of the Centre for Musculoskeletal and Pain Epidemiology, and the Biostatistics and Health Data Science Group as a research fellow, and I’m also working with NHS Grampian as a data scientist. My day-to-day job involves using Electronic Health Care records to answer questions of clinical interest."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Some things I wrote about things I find interesting",
    "section": "",
    "text": "The consequence of ignoring censorship\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2025\n\n\nWarren James\n\n\n\n\n\n\n\n\n\n\n\n\nP-values and Clinical Decisions\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\nWarren James\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "My name’s Warren James and my career in academia started at the university of Aberdeen in the Psychology department. After working for a fast food chain that I will not name (but will say specialises fried chicken from a state in America beginning with K), I was fortunate to be offered the chance to work as a research assistant for a summer project looking at eye movements and decision making. From there, I went on to complete my PhD continuing this train of work. During this time, I spent my days learning about the various ways statistics can be used to help us make sense of complicated data and offer insight into what is happening. Unfortunately, I also saw that “dark side” of statistics when it is used inappropriately leading to some unusual claims that weren’t supported by evidence. As a result of this (and with the firm guidance of my supervisors), I have decided to join the push to raise the standards of research by championing open practices and the use of open-source software to help us all become better researchers. I try to avoid getting on a soap box as much as possible, but like most academics it would appear I like the sound of my own opinion/voice. Having said that, I am always happy to be wrong.\nCurrently, I work in Epidemiolgy as part of the Centre for Musculoskeletal and Pain Epidemiology and the Biostatistics and Health Data Science Group at the University of Aberdeen. In this role, I apply various statistical techniques to answer clinically relevant questions with the hope that this allows us to understand how health care services can be provided to improve outcomes for patients.\nApart from being a number nerd, I also do a bit of music which I’m hoping to start sharing soon. Shameless self promotion on what is meant to be my professional website, but then it is my website so I can plug what I want.\nThis is very much a side project, so I’m hoping it isn’t judged too harshly."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#simulate-visit-data",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#simulate-visit-data",
    "title": "VOICES_keyServiceComponents",
    "section": "Simulate visit data",
    "text": "Simulate visit data\nNow all the values are in the dataset, we can produce count data for the number of visits each patient had during the time they were in the study.\n\nDemonstration\nTo show that this function behaves in the expected way, I have generated two datasets using the bespoke nbinomProcess function and the built in rnbinom function to generate two datasets using the same parameters. The exposure time will be kept constant to allow for an easier comparison to be made with the true distribution.\n\n# set some parameters\nN &lt;- 1e6\nthisMu &lt;- 3.4\nthisSize &lt;- 10.3\n\n# generate data using the rnbinom() function\ndf_rnbinom &lt;- tibble(subj = 1:N, \n                     nEvents = rnbinom(N, thisSize, mu = thisMu), \n                     thisFunction = \"rnbinom\")\n\n# generate data using the nbinomProcess() function\ndf_nbinomProcess &lt;- tibble(subj = 1:N,\n                           thisFunction = \"nbinomProcess\") \ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  nbinomProcess(thisMu, thisSize, 1)\n})\ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  length(df_nbinomProcess$nEvents[[i]])\n})\n\n# create figure to compare \nrbind(df_rnbinom, df_nbinomProcess) %&gt;% \n  ggplot(aes(nEvents, fill = thisFunction)) + \n  geom_histogram(binwidth = 1, \n                 position = \"dodge\") + \n  geom_line(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                          y = dnbinom(x, thisSize, mu = thisMu) * N),\n            aes(x, y), \n            inherit.aes = F) +\n  geom_point(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                           y = dnbinom(x, thisSize, mu = thisMu) * N),\n             aes(x, y), \n             size = 3,\n             inherit.aes = F) +\n  scale_fill_flat()\n\n\n\n\n\n\n\n\nFrom the above, we can see that the functions produce very similar results that both match onto the true distribution with a high enough number of samples. The reason for the nbinomProcess function is to produce a number of events with time stamps to make data that looks similar to what would be available for analysis."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#create-synthetic-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#create-synthetic-dataset",
    "title": "VOICES_keyServiceComponents",
    "section": "Create synthetic dataset",
    "text": "Create synthetic dataset\n\n# generate a series of positive and negative events \n# For the purposes of this dataset, I've made it so that everyone has 3 \"negative\" events\ntoBind &lt;- t(sapply(1:nrow(df_patientData), function(i){\n  time &lt;- df_patientData$timeYears[i]\n  genVisits(3, df_patientData$mu[i], df_patientData$shape[i] * time, time)\n})) \n\n# bind the data and unnest the list columns \ndf_patientData &lt;- df_patientData %&gt;% \n  cbind(toBind) %&gt;% \n  unnest(c(posEvent, EventTime))"
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#format-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#format-dataset",
    "title": "VOICES_keyServiceComponents",
    "section": "Format dataset",
    "text": "Format dataset\nNow we have the event times, we can make the dataset look a touch more realistic.\n\n# end Date of study to get index data for each patient\nendDate &lt;- as.Date(\"2020-10-31\") \n\n# function to convert between numbers and the data\nChangeTime &lt;- function(og_date, thisYears){\n  if(thisYears[1] &lt; 0){\n    sub &lt;- T\n    thisYears &lt;- abs(thisYears)\n  } else {\n    sub &lt;- F\n  }\n  \n  # sort year value\n  floorYears &lt;- floor(thisYears)\n\n  # sort month value\n  thisMonth &lt;- (thisYears - floorYears) * 12\n  floorMonth &lt;- floor(thisMonth)\n  \n  # sort day value\n  thisDay &lt;- thisMonth - floorMonth\n  thisDay &lt;- round(thisDay * 28)\n  \n  # have to do the months as days because it goes wonky with certain dates otherwise \n  if(sub){\n    og_date -(years(floorYears) + days(floorMonth * 28) + days(thisDay)) \n  } else {\n    og_date + (years(floorYears) + days(floorMonth * 28) + days(thisDay))   \n  }\n}\n\n# format the dataset to look more realistic\ndf_patientData &lt;- df_patientData %&gt;% \n  select(-c(Intercept, shape, iter, mu)) %&gt;%\n  mutate(across(c(AdviceandorLed:WaitTimeNewWithinWeek), ~ abs(.x) &gt; 0), \n         indexDate = ChangeTime(endDate, -timeYears), \n         eventDate = ChangeTime(indexDate, EventTime)) %&gt;% \n  select(subj, indexDate, Board, timeYears, everything(), -EventTime)\n\n# add in an mcon column which is based on whether the event was \"positive\"\n# NB: The \"Codes_X\" variables are defined in the markdown document which can be \n#     found in the github repository. \n#     They are the same as the codes seen in the readME file for the repository. \ndf_patientData$mcon &lt;- sapply(1:nrow(df_patientData), function(i){\n  if(df_patientData$posEvent[i]) {\n    sample(Codes_SI, 1)\n  } else {\n    sample(c(Codes_Cancer, Codes_CVD), 1)\n  }\n})\n\n# Add in some other codes\ndf_patientData$ocon &lt;- sapply(1:nrow(df_patientData), function(i){\n  nOthers &lt;- sample(0:5, 1)\n  if(df_patientData$posEvent[i]){\n    output &lt;- sample(c(Codes_SI, Codes_Cancer, Codes_CVD), nOthers)\n  } else {\n    output &lt;- sample(c(Codes_Cancer, Codes_CVD), nOthers)\n  }\n  paste(c(output, rep(\"NA\", 5 - nOthers)), collapse = \"|\")\n})\n\n# tidy up the dataset \ndf_patientData &lt;- df_patientData %&gt;% \n  separate(ocon, into = paste0(\"ocon\", c(1:5)), sep = \"[|]\")\n\n# show the dataset\nreactable(df_patientData)\n\n\n\n\n\nPlease note, the ICD-10 codes that don’t relate to Serious Infection were sample from two other lists regarding Cancer and Cardiovascular Disease so these probably aren’t the most clinically valid. The extra codes are simply there to give an impression of what the real data looks like."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#run-model",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#run-model",
    "title": "VOICES_keyServiceComponents",
    "section": "Run model",
    "text": "Run model\nThe model was fit using the brms library. All variables regarding the service components were entered as main effects. In the paper, additional parameters include Sex (deviation scaled), Age (median centered and decade scaled), Urban/Rural status (2 fold classification), and Scottish Index of Multiple Deprivation (as quintiles with 3 set as the baseline). Additionally, there was a random intercept for board of treatment to account for differences in base rates of Serious Infection. For this example, these haven’t been included. All coefficients regarding the service components (and the demographic features in the paper) were given a weakly informative prior of a student t distribution with a mean of 0, degrees of freedom of 3, and standard deviation of 1. A comparison between the prior predictions and posterior predictions can be seen by fitting the same model with the sample_prior argument set to \"only\".\n\nmy_prior &lt;- prior(student_t(3, 0, 1), class = \"b\")\n\n# A model that only samples the prior \n# I've commented this out because it doesn't need to be run here \n# but should you want to check the prior assumptions, this is the code you need\n# m_nbinomn_priorOnly &lt;- brm(\n#   SI_events | rate(timeYears) ~\n#     AdviceandorLed + CohortedClinic + JointParaClinic + \n#     LocalAAVpath + OwnDayCaseUnit + VascMDT + \n#     WaitTimeNewWithinWeek, \n#   data = df_modelData, \n#   family = negbinomial(), \n#   prior = my_prior,\n#   sample_prior = \"only\",\n#   chains = 1,\n#   iter = 4000, \n#   warmup = 3000,\n#   control = list(adapt_delta = .9)\n# )\n\nm_nbinom &lt;- brm(\n  SI_events | rate(timeYears) ~\n    AdviceandorLed + CohortedClinic + JointParaClinic + \n    LocalAAVpath + OwnDayCaseUnit + VascMDT + \n    WaitTimeNewWithinWeek, \n  data = df_modelData, \n  family = negbinomial(), \n  prior = my_prior, \n  chains = 1,\n  iter = 4000, \n  warmup = 2000,\n  control = list(adapt_delta = .9)\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000848 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.48 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 1: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 7.768 seconds (Warm-up)\nChain 1:                7.393 seconds (Sampling)\nChain 1:                15.161 seconds (Total)\nChain 1: \n\nsummary(m_nbinom)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: SI_events | rate(timeYears) ~ AdviceandorLed + CohortedClinic + JointParaClinic + LocalAAVpath + OwnDayCaseUnit + VascMDT + WaitTimeNewWithinWeek \n   Data: df_modelData (Number of observations: 850) \n  Draws: 1 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    -1.66      0.15    -1.95    -1.37 1.00     1750\nAdviceandorLedTRUE            0.10      0.18    -0.25     0.45 1.00     1436\nCohortedClinicTRUE           -0.15      0.09    -0.32     0.02 1.00     1600\nJointParaClinicTRUE           0.05      0.09    -0.14     0.21 1.00     2229\nLocalAAVpathTRUE             -0.13      0.10    -0.32     0.06 1.00     1728\nOwnDayCaseUnitTRUE            0.11      0.11    -0.10     0.32 1.00     1581\nVascMDTTRUE                  -0.14      0.17    -0.47     0.21 1.00     1562\nWaitTimeNewWithinWeekTRUE    -0.04      0.12    -0.29     0.21 1.00     1851\n                          Tail_ESS\nIntercept                     1593\nAdviceandorLedTRUE            1297\nCohortedClinicTRUE            1514\nJointParaClinicTRUE           1451\nLocalAAVpathTRUE              1405\nOwnDayCaseUnitTRUE            1630\nVascMDTTRUE                   1090\nWaitTimeNewWithinWeekTRUE     1606\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     0.16      0.02     0.13     0.20 1.00     2058     1515\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAbove, the model results can be show in terms of the coefficient values. The first thing to notice is that the Credibility Intervals are more narrow than those of in the paper, but this is primarily due to there being less noise in the estimates associated with patients having varying demographic features and no noise added due to random effects. In essence, this simulated data only allows access to service components to vary with all other features being the same. Generally, the results look very similar to those in the paper despite the limitations."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#show-model-results",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#show-model-results",
    "title": "VOICES_keyServiceComponents",
    "section": "Show model results",
    "text": "Show model results\nSimply looking at the summary isn’t always the easiest way to understand these results, so below I’ve created a few plots to demonstrate make things clearer. The first thing to do is get the posterior draws.\n\npost_nbinom &lt;- m_nbinom %&gt;% \n  as.matrix() %&gt;% \n  as_tibble() %&gt;% \n  select(-c(lp__, lprior)) \n\n\nPlotting results\n\nRaw coefficients\nThis figure shows the raw coefficients on a log scale. With the expection of the Intercept, all values can be compared to the 0 line in order to get a sense for the direction of the effect.\n\n\n\n\n\n\n\n\n\n\n\nIncident Rate Ratios\nThis figure shows the Incident Rate Ratio for each service component. These values correspond to the ratio of the rate between a service component being present and not being present. Values greater than one show an increase in the rate, while values lower than one show a decrease in the rate.\n\n\n\n\n\n\n\n\n\n\n\nResults as rates\nThis figure shows the results as rate values. This way compares how the presence of different service components was associated with changes in the yearly rate of Serious Infection. The vertical line represents the mean Intercept rate with the shaded region showing the 95% credibility interval. Please note, this doesn’t show the range of predicted counts, but the uncertainty in the estimate for the mean parameter (i.e., the mean rate of events in these data)."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#expected-number-of-events-over-time",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#expected-number-of-events-over-time",
    "title": "VOICES_keyServiceComponents",
    "section": "Expected number of events over time",
    "text": "Expected number of events over time\nThis plot shows the total number of expected events over time in the absence of any of the service components (in grey). Each facet shows how this number of events could be expected to change this count compared to having no other service components. The larger shaded region shows where 95% of the data would be expected to lie in a negative binomial distribution using the point estimates from the model outputs. The solid line shows the mean number of events to be expected given the parameter values. The shaded region between the dashed lines shows the 95% credibility interval around the estimate for the mean value.\n\n# poorly named function... please don't do this, make useful function names\n# this function is also really specific to this problem, so it would need to be adapted \n# for any other model/dataset\n# Using the rnbinom() function is susceptible to the upper and lower bounds moving about due to \n# the nature of these data being integers... \nmakePlot &lt;- function(thisModel, minTime, maxTime, res = .02) {\n  post &lt;- thisModel %&gt;% as.matrix() %&gt;% \n    as_tibble() %&gt;% \n    select(-c(lprior, lp__)) %&gt;% \n    mutate(across(2:(ncol(.) - 1), ~ .x + b_Intercept),\n           across(1:(ncol(.) - 1), ~ exp(.x))) %&gt;% \n    gather(1:(ncol(.)), \n           key = \"param\", \n           value = \"coef\") %&gt;% \n    group_by(param) %&gt;% \n    mean_hdci(coef)  %&gt;% \n    expand_grid(time = seq(minTime, maxTime, res)) %&gt;% \n    group_by(param) %&gt;% \n    mutate(iter = row_number()) %&gt;% \n    select(iter, everything()) %&gt;% \n    ungroup() \n  \n  quantile_data &lt;- post %&gt;% \n    select(param, coef, time) %&gt;% \n    spread(param, coef) %&gt;% \n    gather(2:(ncol(.) - 1), \n           key = \"param\", \n           value = \"mu\") %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5 = qnbinom(.5, shape * time, mu = mu * time),\n           q.975 = qnbinom(.975, shape * time, mu = mu * time),\n           q.025 = qnbinom(.025, shape * time, mu = mu * time))\n  \n  mu_post &lt;- post %&gt;% \n    select(-c(.width, .point, .interval)) %&gt;%  \n    filter(!param %in% c(\"shape\", \"b_Intercept\")) %&gt;% \n    left_join(post %&gt;%\n                filter(param == \"shape\") %&gt;% \n                select(iter, coef) %&gt;% \n                rename(shape = coef)) %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5_med = qnbinom(.5, shape * time, mu = coef * time),\n           q.5_upr = qnbinom(.5, shape * time, mu = .upper * time),\n           q.5_lwr = qnbinom(.5, shape * time, mu = .lower * time)) \n  \n  quantile_data[quantile_data$param != \"Intercept\",] %&gt;% \n    ggplot(aes(time, q.5, colour = param)) + \n    geom_path(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n              aes(time, q.5),\n              colour = \"black\") +\n    geom_ribbon(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n                aes(ymin = q.025, ymax = q.975), \n                fill = \"black\", \n                colour = \"transparent\",\n                alpha = .1) + \n    geom_path() + \n    geom_ribbon(aes(ymin = q.025, ymax = q.975, \n                    fill = param), \n                colour = \"transparent\",\n                alpha = .2) +\n    geom_ribbon(data = mu_post,\n                aes(x = time,\n                    ymin = q.5_lwr - .2, ymax = q.5_upr + .2,\n                    y = q.5_med,\n                    fill = param),\n                alpha = .2,\n                linetype = \"longdash\") +\n    scale_x_continuous(\"Time since Index Date\") + \n    scale_y_continuous(\"Estimated total number of Serious Infections\") +\n    facet_wrap(~param) +\n    theme(legend.position = \"bottom\", \n          panel.grid.major.y = element_line())\n  \n  # return(post)\n}\n\nmakePlot(m_nbinom, .1, 24) \n\nJoining with `by = join_by(iter)`"
  },
  {
    "objectID": "supMat/index.html",
    "href": "supMat/index.html",
    "title": "Supplementary Materials",
    "section": "",
    "text": "VOICES: Identidying Key Service Components\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nWarren James\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#simulate-visit-data",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#simulate-visit-data",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Simulate visit data",
    "text": "Simulate visit data\nNow all the values are in the dataset, we can produce count data for the number of visits each patient had during the time they were in the study.\n\nDemonstration\nTo show that this function behaves in the expected way, I have generated two datasets using the bespoke nbinomProcess function and the built in rnbinom function to generate two datasets using the same parameters. The exposure time will be kept constant to allow for an easier comparison to be made with the true distribution.\n\n# set some parameters\nN &lt;- 1e6\nthisMu &lt;- 3.4\nthisSize &lt;- 10.3\n\n# generate data using the rnbinom() function\ndf_rnbinom &lt;- tibble(subj = 1:N, \n                     nEvents = rnbinom(N, thisSize, mu = thisMu), \n                     thisFunction = \"rnbinom\")\n\n# generate data using the nbinomProcess() function\ndf_nbinomProcess &lt;- tibble(subj = 1:N,\n                           thisFunction = \"nbinomProcess\") \ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  nbinomProcess(thisMu, thisSize, 1)\n})\ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  length(df_nbinomProcess$nEvents[[i]])\n})\n\n# create figure to compare \nrbind(df_rnbinom, df_nbinomProcess) %&gt;% \n  ggplot(aes(nEvents, fill = thisFunction)) + \n  geom_histogram(binwidth = 1, \n                 position = \"dodge\") + \n  geom_line(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                          y = dnbinom(x, thisSize, mu = thisMu) * N),\n            aes(x, y), \n            inherit.aes = F) +\n  geom_point(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                           y = dnbinom(x, thisSize, mu = thisMu) * N),\n             aes(x, y), \n             size = 3,\n             inherit.aes = F) +\n  scale_fill_flat()\n\n\n\n\n\n\n\n\nFrom the above, we can see that the functions produce very similar results that both match onto the true distribution with a high enough number of samples. The reason for the nbinomProcess function is to produce a number of events with time stamps to make data that looks similar to what would be available for analysis."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#create-synthetic-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#create-synthetic-dataset",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Create synthetic dataset",
    "text": "Create synthetic dataset\n\n# generate a series of positive and negative events \n# For the purposes of this dataset, I've made it so that everyone has 3 \"negative\" events\ntoBind &lt;- t(sapply(1:nrow(df_patientData), function(i){\n  time &lt;- df_patientData$timeYears[i]\n  genVisits(3, df_patientData$mu[i], df_patientData$shape[i] * time, time)\n})) \n\n# bind the data and unnest the list columns \ndf_patientData &lt;- df_patientData %&gt;% \n  cbind(toBind) %&gt;% \n  unnest(c(posEvent, EventTime))"
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#format-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#format-dataset",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Format dataset",
    "text": "Format dataset\nNow we have the event times, we can make the dataset look a touch more realistic.\n\n# end Date of study to get index data for each patient\nendDate &lt;- as.Date(\"2020-10-31\") \n\n# function to convert between numbers and the data\nChangeTime &lt;- function(og_date, thisYears){\n  if(thisYears[1] &lt; 0){\n    sub &lt;- T\n    thisYears &lt;- abs(thisYears)\n  } else {\n    sub &lt;- F\n  }\n  \n  # sort year value\n  floorYears &lt;- floor(thisYears)\n\n  # sort month value\n  thisMonth &lt;- (thisYears - floorYears) * 12\n  floorMonth &lt;- floor(thisMonth)\n  \n  # sort day value\n  thisDay &lt;- thisMonth - floorMonth\n  thisDay &lt;- round(thisDay * 28)\n  \n  # have to do the months as days because it goes wonky with certain dates otherwise \n  if(sub){\n    og_date -(years(floorYears) + days(floorMonth * 28) + days(thisDay)) \n  } else {\n    og_date + (years(floorYears) + days(floorMonth * 28) + days(thisDay))   \n  }\n}\n\n# format the dataset to look more realistic\ndf_patientData &lt;- df_patientData %&gt;% \n  select(-c(Intercept, shape, iter, mu)) %&gt;%\n  mutate(across(c(AdviceandorLed:WaitTimeNewWithinWeek), ~ abs(.x) &gt; 0), \n         indexDate = ChangeTime(endDate, -timeYears), \n         eventDate = ChangeTime(indexDate, EventTime)) %&gt;% \n  select(subj, indexDate, Board, timeYears, everything(), -EventTime)\n\n# add in an mcon column which is based on whether the event was \"positive\"\n# NB: The \"Codes_X\" variables are defined in the markdown document which can be \n#     found in the github repository. \n#     They are the same as the codes seen in the readME file for the repository. \ndf_patientData$mcon &lt;- sapply(1:nrow(df_patientData), function(i){\n  if(df_patientData$posEvent[i]) {\n    sample(Codes_SI, 1)\n  } else {\n    sample(c(Codes_Cancer, Codes_CVD), 1)\n  }\n})\n\n# Add in some other codes\ndf_patientData$ocon &lt;- sapply(1:nrow(df_patientData), function(i){\n  nOthers &lt;- sample(0:5, 1)\n  if(df_patientData$posEvent[i]){\n    output &lt;- sample(c(Codes_SI, Codes_Cancer, Codes_CVD), nOthers)\n  } else {\n    output &lt;- sample(c(Codes_Cancer, Codes_CVD), nOthers)\n  }\n  paste(c(output, rep(\"NA\", 5 - nOthers)), collapse = \"|\")\n})\n\n# tidy up the dataset \ndf_patientData &lt;- df_patientData %&gt;% \n  separate(ocon, into = paste0(\"ocon\", c(1:5)), sep = \"[|]\")\n\n# show the dataset\nreactable(df_patientData)\n\n\n\n\n\nPlease note, the ICD-10 codes that don’t relate to Serious Infection were sample from two other lists regarding Cancer and Cardiovascular Disease so these probably aren’t the most clinically valid. The extra codes are simply there to give an impression of what the real data looks like."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#run-model",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#run-model",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Run model",
    "text": "Run model\nThe model was fit using the brms library. All variables regarding the service components were entered as main effects. In the paper, additional parameters include Sex (deviation scaled), Age (median centered and decade scaled), Urban/Rural status (2 fold classification), and Scottish Index of Multiple Deprivation (as quintiles with 3 set as the baseline). Additionally, there was a random intercept for board of treatment to account for differences in base rates of Serious Infection. For this example, these haven’t been included. All coefficients regarding the service components (and the demographic features in the paper) were given a weakly informative prior of a student t distribution with a mean of 0, degrees of freedom of 3, and standard deviation of 1. A comparison between the prior predictions and posterior predictions can be seen by fitting the same model with the sample_prior argument set to \"only\".\n\nmy_prior &lt;- prior(student_t(3, 0, 1), class = \"b\")\n\n# A model that only samples the prior \n# I've commented this out because it doesn't need to be run here \n# but should you want to check the prior assumptions, this is the code you need\n# m_nbinomn_priorOnly &lt;- brm(\n#   SI_events | rate(timeYears) ~\n#     AdviceandorLed + CohortedClinic + JointParaClinic + \n#     LocalAAVpath + OwnDayCaseUnit + VascMDT + \n#     WaitTimeNewWithinWeek, \n#   data = df_modelData, \n#   family = negbinomial(), \n#   prior = my_prior,\n#   sample_prior = \"only\",\n#   chains = 1,\n#   iter = 4000, \n#   warmup = 3000,\n#   control = list(adapt_delta = .9)\n# )\n\nm_nbinom &lt;- brm(\n  SI_events | rate(timeYears) ~\n    AdviceandorLed + CohortedClinic + JointParaClinic + \n    LocalAAVpath + OwnDayCaseUnit + VascMDT + \n    WaitTimeNewWithinWeek, \n  data = df_modelData, \n  family = negbinomial(), \n  prior = my_prior, \n  chains = 1,\n  iter = 4000, \n  warmup = 2000,\n  control = list(adapt_delta = .9)\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.00048 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.8 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 1: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 8.195 seconds (Warm-up)\nChain 1:                19.736 seconds (Sampling)\nChain 1:                27.931 seconds (Total)\nChain 1: \n\nsummary(m_nbinom)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: SI_events | rate(timeYears) ~ AdviceandorLed + CohortedClinic + JointParaClinic + LocalAAVpath + OwnDayCaseUnit + VascMDT + WaitTimeNewWithinWeek \n   Data: df_modelData (Number of observations: 850) \n  Draws: 1 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    -1.66      0.15    -1.95    -1.37 1.00     1750\nAdviceandorLedTRUE            0.10      0.18    -0.25     0.45 1.00     1436\nCohortedClinicTRUE           -0.15      0.09    -0.32     0.02 1.00     1600\nJointParaClinicTRUE           0.05      0.09    -0.14     0.21 1.00     2229\nLocalAAVpathTRUE             -0.13      0.10    -0.32     0.06 1.00     1728\nOwnDayCaseUnitTRUE            0.11      0.11    -0.10     0.32 1.00     1581\nVascMDTTRUE                  -0.14      0.17    -0.47     0.21 1.00     1562\nWaitTimeNewWithinWeekTRUE    -0.04      0.12    -0.29     0.21 1.00     1851\n                          Tail_ESS\nIntercept                     1593\nAdviceandorLedTRUE            1297\nCohortedClinicTRUE            1514\nJointParaClinicTRUE           1451\nLocalAAVpathTRUE              1405\nOwnDayCaseUnitTRUE            1630\nVascMDTTRUE                   1090\nWaitTimeNewWithinWeekTRUE     1606\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     0.16      0.02     0.13     0.20 1.00     2058     1515\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAbove, the model results can be show in terms of the coefficient values. The first thing to notice is that the Credibility Intervals are more narrow than those of in the paper, but this is primarily due to there being less noise in the estimates associated with patients having varying demographic features and no noise added due to random effects. In essence, this simulated data only allows access to service components to vary with all other features being the same. Generally, the results look very similar to those in the paper despite the limitations."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#show-model-results",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#show-model-results",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Show model results",
    "text": "Show model results\nSimply looking at the summary isn’t always the easiest way to understand these results, so below I’ve created a few plots to demonstrate make things clearer. The first thing to do is get the posterior draws.\n\npost_nbinom &lt;- m_nbinom %&gt;% \n  as.matrix() %&gt;% \n  as_tibble() %&gt;% \n  select(-c(lp__, lprior)) \n\n\nPlotting results\n\nRaw coefficients\nThis figure shows the raw coefficients on a log scale. With the expection of the Intercept, all values can be compared to the 0 line in order to get a sense for the direction of the effect.\n\n\n\n\n\n\n\n\n\n\n\nIncident Rate Ratios\nThis figure shows the Incident Rate Ratio for each service component. These values correspond to the ratio of the rate between a service component being present and not being present. Values greater than one show an increase in the rate, while values lower than one show a decrease in the rate.\n\n\n\n\n\n\n\n\n\n\n\nResults as rates\nThis figure shows the results as rate values. This way compares how the presence of different service components was associated with changes in the yearly rate of Serious Infection. The vertical line represents the mean Intercept rate with the shaded region showing the 95% credibility interval. Please note, this doesn’t show the range of predicted counts, but the uncertainty in the estimate for the mean parameter (i.e., the mean rate of events in these data)."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#expected-number-of-events-over-time",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#expected-number-of-events-over-time",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Expected number of events over time",
    "text": "Expected number of events over time\nThis plot shows the total number of expected events over time in the absence of any of the service components (in grey). Each facet shows how this number of events could be expected to change this count compared to having no other service components. The larger shaded region shows where 95% of the data would be expected to lie in a negative binomial distribution using the point estimates from the model outputs. The solid line shows the mean number of events to be expected given the parameter values. The shaded region between the dashed lines shows the 95% credibility interval around the estimate for the mean value.\n\n# poorly named function... please don't do this, make useful function names\n# this function is also really specific to this problem, so it would need to be adapted \n# for any other model/dataset\n# Using the rnbinom() function is susceptible to the upper and lower bounds moving about due to \n# the nature of these data being integers... \nmakePlot &lt;- function(thisModel, minTime, maxTime, res = .02) {\n  post &lt;- thisModel %&gt;% as.matrix() %&gt;% \n    as_tibble() %&gt;% \n    select(-c(lprior, lp__)) %&gt;% \n    mutate(across(2:(ncol(.) - 1), ~ .x + b_Intercept),\n           across(1:(ncol(.) - 1), ~ exp(.x))) %&gt;% \n    gather(1:(ncol(.)), \n           key = \"param\", \n           value = \"coef\") %&gt;% \n    group_by(param) %&gt;% \n    mean_hdci(coef)  %&gt;% \n    expand_grid(time = seq(minTime, maxTime, res)) %&gt;% \n    group_by(param) %&gt;% \n    mutate(iter = row_number()) %&gt;% \n    select(iter, everything()) %&gt;% \n    ungroup() \n  \n  quantile_data &lt;- post %&gt;% \n    select(param, coef, time) %&gt;% \n    spread(param, coef) %&gt;% \n    gather(2:(ncol(.) - 1), \n           key = \"param\", \n           value = \"mu\") %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5 = qnbinom(.5, shape * time, mu = mu * time),\n           q.975 = qnbinom(.975, shape * time, mu = mu * time),\n           q.025 = qnbinom(.025, shape * time, mu = mu * time))\n  \n  mu_post &lt;- post %&gt;% \n    select(-c(.width, .point, .interval)) %&gt;%  \n    filter(!param %in% c(\"shape\", \"b_Intercept\")) %&gt;% \n    left_join(post %&gt;%\n                filter(param == \"shape\") %&gt;% \n                select(iter, coef) %&gt;% \n                rename(shape = coef)) %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5_med = qnbinom(.5, shape * time, mu = coef * time),\n           q.5_upr = qnbinom(.5, shape * time, mu = .upper * time),\n           q.5_lwr = qnbinom(.5, shape * time, mu = .lower * time)) \n  \n  quantile_data[quantile_data$param != \"Intercept\",] %&gt;% \n    ggplot(aes(time, q.5, colour = param)) + \n    geom_path(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n              aes(time, q.5),\n              colour = \"black\") +\n    geom_ribbon(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n                aes(ymin = q.025, ymax = q.975), \n                fill = \"black\", \n                colour = \"transparent\",\n                alpha = .1) + \n    geom_path() + \n    geom_ribbon(aes(ymin = q.025, ymax = q.975, \n                    fill = param), \n                colour = \"transparent\",\n                alpha = .2) +\n    geom_ribbon(data = mu_post,\n                aes(x = time,\n                    ymin = q.5_lwr - .2, ymax = q.5_upr + .2,\n                    y = q.5_med,\n                    fill = param),\n                alpha = .2,\n                linetype = \"longdash\") +\n    scale_x_continuous(\"Time since Index Date\") + \n    scale_y_continuous(\"Estimated total number of Serious Infections\") +\n    facet_wrap(~param) +\n    theme(legend.position = \"bottom\", \n          panel.grid.major.y = element_line())\n  \n  # return(post)\n}\n\nmakePlot(m_nbinom, .1, 24) \n\nJoining with `by = join_by(iter)`"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html",
    "title": "P-values and Clinical Decisions",
    "section": "",
    "text": "I was asked by an old master’s student for some advice about presenting the results of some work they had done looking at readmission amongst people discharged to a ward vs. being sent home. Obviously, we can appreciate that there may be some differences in terms of the outcomes between people sent home and those remaining in hospital. While I was very excited to see him using R for his analysis and Rmarkdown to show his working, something else didn’t sit well with me. His question was whether there was any “significant” difference in the probability of these people being readmitted to hospital 30 days after being discharged.\nI have no qualms with discussing the problems of significance testing in a host of different contexts (follow this link for a fantastic discussion of inappropriate applications of significance testing), and as such this grabbed my attention. The approach I prefer to take is one that looks at what we could reasonably surmise given the data that is in front of us. Therefore, I prefer to employ Bayesian methods to answer most (scientific) questions I’m presented with.\nBeing the pedant I am, I took the work and offered an alternative perspective that I believe safeguarded against some of the issues that are mentioned in the paper I referenced above. The aim of this isn’t to discourage people from using p-values, but to try and anticipate how your research would be used by stakeholders. If they aren’t likely to understand the nuances of Null Hypothesis Significance Testing, or may misinterpret the results, then maybe it’s not the best idea to use these tools.\n\n\nHere are the packages I used for this post:\n\nlibrary(tidyverse) # data processing\nlibrary(tidybayes) # for tidy format model draws\nlibrary(brms)      # Bayesian regression\nlibrary(see)       # prettier colour schemes\nlibrary(ggridges)  # a nice package for nice plots\n\n# set the theme \ntheme_set(theme_bw() + \n            theme(legend.position = \"bottom\"))\n\n# for reproducibility \nset.seed(970431)\n\n# The colours I used for any plots \nmyColours &lt;- c(\"#c62828\", \"#283593\", \"#2e7d32\", \"#f9a825\")"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#packages-used",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#packages-used",
    "title": "P-values and Clinical Decisions",
    "section": "",
    "text": "Here are the packages I used for this post:\n\nlibrary(tidyverse) # data processing\nlibrary(tidybayes) # for tidy format model draws\nlibrary(brms)      # Bayesian regression\nlibrary(see)       # prettier colour schemes\nlibrary(ggridges)  # a nice package for nice plots\n\n# set the theme \ntheme_set(theme_bw() + \n            theme(legend.position = \"bottom\"))\n\n# for reproducibility \nset.seed(970431)\n\n# The colours I used for any plots \nmyColours &lt;- c(\"#c62828\", \"#283593\", \"#2e7d32\", \"#f9a825\")"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty",
    "title": "P-value and Clinical Decisions",
    "section": "visualising the uncertainty",
    "text": "visualising the uncertainty\nFirst, I considered that the dataset is very small. This raised the question of: what sort of probability values could reasonably produce data like these? Making the assumption that a beta distribution could suitably represent these data, I produced a plot to perhaps answer this question.\n\nx &lt;- seq(0, .3, length.out = 100)\ndf_beta &lt;- df_fisher %&gt;% \n  expand_grid(tibble(x = x)) %&gt;% \n  mutate(dens = dbeta(x, totalReadmit, totalNoReadmit)) \n\ndf_beta %&gt;% \n  ggplot(aes(x, dens, colour = Group)) + \n  geom_path() \n\n\n\n\n\n\n\n\nWe can see that there’s a large degree of overlap in these two distributions. However a quick visual inspection would suggest that the Ward group was more likely to not be readmitted given the majority of the curve is lower than the Home group. Given we have such small numbers, it is unsurprising we didn’t observe a significant difference.\nTherefore, I felt it best to offer a Bayesian approach. The way I viewed these data is that for 51 people, we have a record or whether they were readmitted after being discharged, and a grouping variable of where they were discharged to. So this seemed like a straightforward logistic regression problem"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#model-fitting",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#model-fitting",
    "title": "P-values and Clinical Decisions",
    "section": "Model fitting",
    "text": "Model fitting\nFirst thing’s first, I fit a model using the default priors from the brms package.\n\nm_defaultPriors &lt;- brm(readmit ~ Group, \n                       data = df_readmission, \n                       chains = 1,\n                       family = \"bernoulli\", \n                       sample_prior = T, \n                       seed = 987421)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 1:                0.032 seconds (Sampling)\nChain 1:                0.06 seconds (Total)\nChain 1: \n\n# show model summary \nsummary(m_defaultPriors)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: readmit ~ Group \n   Data: df_readmission (Number of observations: 51) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -3.35      0.99    -5.55    -1.81 1.01      427      433\nGroupHome     0.79      1.29    -1.68     3.40 1.00      403      489\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLike most people, I’m not able to transform these values onto a scale that makes sense off the top of my head. So, I’ve made a quick function that will summarise the the outputs on a ratio scale instead. Note, there’s almost definitely a neater function out there, but we’re not here to make the prettiest most efficient functions.\n\n# quick function as we'll be doing this again \nmyOddsRatio &lt;- function(thisModel){\n  as_draws_df(thisModel) %&gt;% \n    select(b_GroupHome) %&gt;% \n    mean_hdci(b_GroupHome, .width = c(.95)) %&gt;% \n    mutate(across(1:3, ~exp(.x)))\n}\n\nmyOddsRatio(m_defaultPriors) \n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 1 × 6\n  b_GroupHome .lower .upper .width .point .interval\n        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1        2.21  0.166   26.2   0.95 mean   hdci     \n\n\nFrom the above output, we can see there is still a very large amount of uncertainty about this estimate. However, the Bayesian model is a bit more certain than the original Fisher test. For those that need more convincing, take a look at Figure 2 and you can clearly see that the Bayesian model offers a more precise estimate.\n\n\nCode\n# setup dataset for use later\ndf_comparison &lt;- tibble(\n  testType = c(\"Fisher\",\n               \"Bayes_defaultPriors\"), \n  mu       = c(fisherResults$estimate,\n               exp(fixef(m_defaultPriors)[2, 1])), \n  .lower   = c(fisherResults$conf.int[1], \n               exp(fixef(m_defaultPriors)[2, 3])), \n  .upper   = c(fisherResults$conf.int[2], \n               exp(fixef(m_defaultPriors)[2, 4]))\n) %&gt;% \n  mutate(testType = factor(testType, levels = c(\"Fisher\", \"Bayes_defaultPriors\")))\n\ndf_comparison %&gt;% \n  ggplot(aes(mu, testType, colour = testType)) + \n  geom_vline(xintercept = 1) +\n  geom_point() + \n  geom_linerange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_log10(\"Odds Ratio\") + \n  scale_y_discrete(\"\") + \n  scale_colour_manual(values = myColours)\n\n\n\n\n\n\n\n\nFigure 2: Odds Ratio for the likelihood of readmission to hospital when being discharged ‘home’ compared to a ‘ward’. The x-axis is on a log10 scaled for clarity.\n\n\n\n\n\nAlthough the Bayesian model estimates are less uncertain, we still have a large degree of uncertainty in our estimate. While this may simply be a result of having a small dataset, there are other things we should also consider when fitting these models. Given we’re using Bayesian stats now, we have the luxury of adding prior beliefs to our model. While priors can be subjective, we have a very real reason to not rely exclusively on default (or, even worse, uniform) priors when dealing with logistic regression."
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty-in-the-data",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty-in-the-data",
    "title": "P-values and Clinical Decisions",
    "section": "visualising the uncertainty in the data",
    "text": "visualising the uncertainty in the data\nFirst, I considered that the dataset is very small. This raised the question of: what sort of probability values could reasonably produce data like these? Making the assumption that a beta distribution could suitably represent these data, I produced a plot to perhaps answer this question.\n\nx &lt;- seq(0, .3, length.out = 100)\ndf_beta &lt;- df_fisher %&gt;% \n  expand_grid(tibble(x = x)) %&gt;% \n  mutate(dens = dbeta(x, totalReadmit, totalNoReadmit)) \n\ndf_beta %&gt;% \n  ggplot(aes(x, dens, colour = Group)) + \n  geom_path() + \n  scale_colour_manual(values = myColours)\n\n\n\n\n\n\n\nFigure 1: This figure shows the likelihood of various values that could have produced the observed data.\n\n\n\n\n\nWe can see that there’s a large degree of overlap in these two distributions. However a quick visual inspection would suggest that the Ward group was more likely to not be readmitted given the majority of the curve is lower than the Home group. Given we have such small numbers, it is unsurprising we didn’t observe a significant difference.\nTherefore, I felt it best to offer a Bayesian approach. The way I viewed these data is that for 51 people, we have a record or whether they were readmitted after being discharged, and a grouping variable of where they were discharged to. So this seemed like a straightforward logistic regression problem"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#more-informative-priors",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#more-informative-priors",
    "title": "P-values and Clinical Decisions",
    "section": "More informative priors",
    "text": "More informative priors\n\nInitial beliefs\nWe can retrieve the default priors from the model we implemented by using the handy function get_prior.\n\nget_prior(readmit ~ Group, \n          data = df_readmission,\n          family = \"bernoulli\")\n\n                prior     class      coef group resp dpar nlpar lb ub\n               (flat)         b                                      \n               (flat)         b GroupHome                            \n student_t(3, 0, 2.5) Intercept                                      \n       source\n      default\n (vectorized)\n      default\n\n\nIn this case, we have flat priors on the coefficient for the effect of being discharged home. In effect, this means that every value from \\(-\\infty\\) to \\(\\infty\\). Which doesn’t sound very reasonable to me. However, this becomes a particular issue when handling models looking at probability. When we use the inverse logistic function on this belief (to put the values onto a probability scale), we see something undesirable happen.\n\n# get random numbers from some range \nx &lt;- runif(1e5, -5, 5)\n\n# perform the inverse logit function\ninv_x &lt;- plogis(x)\n\n# plot this\ntibble(p = inv_x) %&gt;% \n  ggplot(aes(p)) + \n  geom_histogram(aes(y = ..density..))\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 3: Inverse logit transformation of randomly sampled numbers between -5 and 5.\n\n\n\n\n\nFor this example, we only used numbers between -5 and 5, which occupies an incredibly small region of the range \\(-\\infty\\) to \\(\\infty\\), and we can already see why using priors such as these may cause issues. What the figure shows is that there is much stronger initial belief in values that are at either extreme, and so this will lead to a poorer reflection of what can actually be seen in the data. If we were to use a different prior, we could perform the same steps to look at what the prior would look like. For example, what happens if we use a Student T distribution using the same values for the intercept term in the model we already ran?\n\nx_student &lt;- rstudent_t(n = 1e5, df = 3, mu = 0, sigma = 2.5)\n\ntibble(x_student = plogis(x_student)) %&gt;% \n  ggplot(aes(x_student)) + \n  geom_histogram(aes(y = ..density..))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 4: Inverse logit transformation of randomly sampled numbers from a Student t distribution with the parameters at df = 3, mu = 0, sigma = 2.5.\n\n\n\n\n\nLooks a little bit better, but what if we made the standard deviation smaller?\n\nx_student &lt;- rstudent_t(n = 1e5, df = 3, mu = 0, sigma = 1)\n\ntibble(x_student = plogis(x_student)) %&gt;% \n  ggplot(aes(x_student)) + \n  geom_histogram(aes(y = ..density..))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 5: Inverse logit transformation of randomly sampled numbers from a Student t distribution with the parameters at df = 3, mu = 0, sigma = 1.\n\n\n\n\n\nThis looks like a much more reasonable initial assumption; not only are the extremes represented to a lesser extent, there is also a smaller difference in the density between the lowest and highest points. Playing around with these values is encouraged to see how the prior belief changes, but hopefully this quick demonstration gives some insight into why this may cause a problem.\n\n\nrefitting the model\nNow we have an understanding of our model’s prior beliefs, and what these would mean for the results, we can use this to inform in a model and see how the results change.\n\n# add in somewhat informative priors \nm_studentPrior_sigma1 &lt;- update(m_defaultPriors, \n                                prior = c(prior(student_t(3, 0, 1), class = \"b\"), \n                                          prior(student_t(3, 0, 1), class = \"intercept\")))\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.3 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 1:                0.03 seconds (Sampling)\nChain 1:                0.057 seconds (Total)\nChain 1: \n\nsummary(m_studentPrior_sigma1)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: readmit ~ Group \n   Data: df_readmission (Number of observations: 51) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -3.06      0.78    -4.77    -1.73 1.01      480      413\nGroupHome     0.39      0.84    -1.17     2.17 1.00      605      568\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we can add these results to our plot from before and see what this has done to the uncertainty in our estimates:\n\n\n\n\n\n\n\n\nFigure 6: Figure with the student t priors added to both the intercept and home coefficient.\n\n\n\n\n\nFrom Figure 6, we can now see that the uncertainty in our estimate is reduced by quite a substantial amount. Something else that might be beneficial for a model fit is to change the “mean” value in the prior. At the moment, we’ve centred the prior on a value of 50% which would seem to be a lot higher than the actual data suggests. In this case, we might try something on the lower end to compare the model results. For example, maybe we think there is a 10% change people would be readmitted on average.\n\n# rerun the model with the intercept as 10% \nm_studentPrior_10percent &lt;- update(m_defaultPriors, \n                                   prior = c(prior(student_t(3, 0, 1), class = \"b\"), \n                                             prior(student_t(3, qlogis(.1), 1), class = \"intercept\")))\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 1:                0.035 seconds (Sampling)\nChain 1:                0.069 seconds (Total)\nChain 1: \n\nsummary(m_studentPrior_10percent)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: readmit ~ Group \n   Data: df_readmission (Number of observations: 51) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -3.07      0.81    -4.79    -1.72 1.00      527      551\nGroupHome     0.43      0.86    -1.10     2.23 1.00      720      415\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ndf_comparison &lt;- df_comparison %&gt;% \n  bind_rows(\n    tibble(\n      testType = c(\"Bayes_studentT_10percent\"), \n      mu       = c(exp(fixef(m_studentPrior_10percent)[2, 1])), \n      .lower   = c(exp(fixef(m_studentPrior_10percent)[2, 3])), \n      .upper   = c(exp(fixef(m_studentPrior_10percent)[2, 4]))\n    )\n  ) %&gt;% \n  mutate(testType = factor(testType, levels = c(\"Fisher\", \"Bayes_defaultPriors\", \n                                                \"Bayes_studentT_sigma1\", \n                                                \"Bayes_studentT_10percent\")))\n\ndf_comparison %&gt;% \n  ggplot(aes(mu, testType, colour = testType)) + \n  geom_vline(xintercept = 1) +\n  geom_point() + \n  geom_linerange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_log10(\"Odds Ratio\") + \n  scale_y_discrete(\"\") + \n  scale_colour_manual(values = myColours)\n\n\n\n\n\n\n\nFigure 7: Figure with the student t priors added to both the intercept and home coefficient. The prior for the intercept now has a mean of 10%.\n\n\n\n\n\nHere, we don’t see much of a change in the Ratio, but there is a small gain in terms of certainty. Though, perhaps we’re not showing the model results in a way that is best suited to answering this question."
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#another-way-to-show-the-data",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#another-way-to-show-the-data",
    "title": "P-values and Clinical Decisions",
    "section": "Another way to show the data",
    "text": "Another way to show the data\nAdmittedly, I have somewhat comitted a sin with the above figures, but I do have an explanation. Bayesian statistics isn’t pre-occupied with what IS and ISN’T significant, but rather more interested in what conclusions could reasonably be supported by the data. In showing only the 95% intervals, we’re hiding some of the information that the model is communicating to us. The reason I did this is because we have a mixture of Frequentist and Bayesian methods being applied and shown in the one figure, and I wanted to draw comparisons to the initial analysis.\nHowever, I think this pushes us towards the binary thinking that is often encourage by things like p-values and as such I think these figures should be avoided if possible. Additionally, if we circle back to the link I shared earlier which discusses a few examples of p-values being used incorrectly, figures like these run the risk of encouraging errors in judging what the results show. The risk we run is that the end user will look at these plots and wrongly infer: “The interval contains 1 (no effect), and so there is no difference between the groups, and therefore we should send everyone home”. This isn’t what the analysis shows, however it’s not impossible to imagine people mistakenly making this judgement. The way I see it, it is our job to try and avoid people making these incorrect statements and the best method for doing this is to clearly communicate the model results in a way that is more readily understood. Thankfully, Credibility Intervals make more intuitive sense, so we can use this to our advantage when plotting model results.\nIn this case, I opted to avoid the use of odds ratios and instead plot the predicted probability that someone would be readmitted given they were discharged home vs. to a ward.\n\n\nCode\n# function to get posterior summaries for these data\n# again, not very pretty\ngetPost &lt;- \\(listModels){\n  output &lt;- tibble()\n  for(i in 1:length(listModels)){\n    output &lt;- bind_rows(output, \n                        as_draws_df(listModels[[i]]) %&gt;% \n                          select(1:2) %&gt;% \n                          mutate(modelName = names(listModels)[i]) %&gt;% \n                          rowid_to_column(var = \"iter\"))\n  }\n  \n  return(output %&gt;% \n           mutate(b_GroupHome = b_Intercept + b_GroupHome, \n                  across(c(2:3), ~exp(.x))) %&gt;% \n           gather(2:3, \n                  key = \"param\", \n                  value = \"coef\") %&gt;% \n           mutate(dischargeTo = ifelse(param == \"b_Intercept\", \"Ward\", \"Home\")))\n}\n\ngetPost(list(default = m_defaultPriors, \n             studentT = m_studentPrior_sigma1, \n             tenPercent = m_studentPrior_10percent)) %&gt;% \n  ggplot(aes(dischargeTo, coef, colour = dischargeTo, fill = dischargeTo)) + \n  stat_pointinterval(point_interval = \"median_hdci\", \n                     .width = c(.5, .95), \n                     show.legend = F) + \n  stat_slabinterval(alpha = .3) +\n  # add in a crude estimate from the raw data\n  geom_point(data = df_readmission %&gt;% \n               rename(dischargeTo = Group) %&gt;% \n               group_by(dischargeTo) %&gt;% \n               summarise(N = n(), \n                         n = sum(readmit)) %&gt;% \n               mutate(perc = n/N), \n             aes(y = perc), \n             size = 5, pch = 1, \n             show.legend = F) +\n  facet_wrap(~modelName) + \n  scale_colour_manual(values = myColours) + \n  scale_fill_manual(values = myColours) + \n  scale_y_continuous(\"\", labels = scales::percent_format()) + \n  scale_x_discrete(\"\")\n\n\n\n\n\n\n\n\nFigure 8: Figure showing the entire posterior for the Bayesian models. The solid point shows the median posterior estimate with the 50% (thick) and 95% (thin) credibility intervals as lines. The hollow points show the crude estimate calculated directly from the data.\n\n\n\n\n\nThe benefit of figure 8 is that we can now see the range of values that would be supported by these data. Although we can’t clearly estimate the magnitude of difference, what we can see is that the data support higher probabilities of readmissions for people discharged home than to a ward. Although this isn’t always the case, we can see that there is some reason to believe these people may be at more risk, though we would want more data in order to be even more certain. By coupling this figure with the previous figures looking at the odds Ratios (Figure 7), we can get a more complete picture of what is happening. The odds ratios suggest an increase (albeit uncertain) in the probability of being readmitted when discharged home, while the entire posterior shows that the risk of readmission is relatively low in both groups. In having the entire posterior visible, we can more readily make intuitive judgements about what the model is showing without having to rely on measures like p-values.\nMy thoughts are that the p-value in and of itself isn’t a very useful number to take away. It doesn’t really help us to understand what is happening in the data, but instead is a somewhat simplified “answer” to what are usually complicated questions. In showing the uncertainty in our estimates (either the ratio, or the probability of readmission), we can see that the data are more supportive of there being a small difference between the outcomes. However, the most clear take away here is that we could use more data to inform our estimates."
  },
  {
    "objectID": "blog/2025_03_12_LogisticOnCensoredData/index.html",
    "href": "blog/2025_03_12_LogisticOnCensoredData/index.html",
    "title": "The consequence of ignoring censorship",
    "section": "",
    "text": "The title is perhaps misleading, I’m not about to discuss anything about censorship in terms of politics. For better or worse, this is a post about censorship in the statistical sense of the word. So, depending on your view point, this is maybe a more exciting topic? Maybe it doesn’t quite make people feel quite as animated, but then again maybe it should. Censorship in statistics is a genuinely interesting problem to consider, and has some fairly important consequences if ignored or not handled with care."
  },
  {
    "objectID": "blog/2025_03_12_LogisticOnCensoredData/index.html#library",
    "href": "blog/2025_03_12_LogisticOnCensoredData/index.html#library",
    "title": "The consequence of ignoring censorship",
    "section": "Library",
    "text": "Library\nA pretty standard set of packages, it’s not like we’re doing anything insane just now.\n\nlibrary(tidyverse) # process data\nlibrary(tidybayes) # get model draws in a tidy format\nlibrary(brms)      # fit bayesian regression models\nlibrary(see)       # colour palette\n\n# set the theme\ntheme_set(theme_bw() + \n            theme(legend.position = \"bottom\"))"
  },
  {
    "objectID": "blog/2025_03_12_LogisticOnCensoredData/index.html#simulate-some-data",
    "href": "blog/2025_03_12_LogisticOnCensoredData/index.html#simulate-some-data",
    "title": "The consequence of ignoring censorship",
    "section": "Simulate some data",
    "text": "Simulate some data\nI’m going to use a Weibull distribution as the basis for my synthetic data. This is often used in survival analysis as it has a fully parametric form and can be very useful. There are plenty of others to choose from, but I like this one. You should be able to follow this and simply replace the word “Weibull” with any other distribution that is used in survival analysis.\nThe simulated dataset will have two groups that we want to compare to each other, with one group being more likely to experience the event earlier. For now, we’ll imagine that we’re looking at people who have a condition versus those that don’t, and their risk of death. We can get into why not applying a survival analysis here would lead to weird conclusions at some point, but for now, we can just make the data.\n\n# set seed for reproducibility\nset.seed(87651)\n\n# setup data\nN &lt;- 400\n\n# setup parameters for cases and controls\ncont_shape &lt;- 2\ncont_scale &lt;- 10\n\ncase_shape &lt;- .9\ncase_scale &lt;- 10\n\nSo that’s all we need, what we can do now is have a look at what the true distributions look like.\n\n\nCode\nplt_trueSurv &lt;- tibble(survTime = seq(.25, 30, .25)) %&gt;% \n  mutate(case = dweibull(survTime, case_shape, case_scale), \n         control = dweibull(survTime, cont_shape, cont_scale)) %&gt;% \n  gather(case:control, \n         key = \"group\", \n         value = \"p\") %&gt;% \n  mutate(group = factor(group, levels = c(\"control\", \"case\"))) %&gt;% \n  ggplot(aes(survTime, p, colour = group)) + \n  geom_path() + \n  scale_colour_flat()\nplt_trueSurv\n\n\n\n\n\n\n\n\nFigure 2: The true distribution for our two cohorts in terms of their survival times\n\n\n\n\n\nLooks alright, the controls have a distribution that is more to the right while the cases are to the left. Essentially, cases don’t live as long as controls. So now let’s generate some random data\n\ndf_model &lt;- tibble(subj = 1:(N * 2), \n                   group = rep(c(\"control\", \"case\"), each = N), \n                   trueSurv = c(rweibull(N, cont_shape, cont_scale), rweibull(N, case_shape, case_scale))) %&gt;% \n  mutate(group = factor(group, levels = c(\"control\", \"case\")))\n\nNow we can overlay this onto our true distribution and see what that looks like:\n\n\nCode\nplt_trueSurv_withHist &lt;- plt_trueSurv + \n  geom_histogram(data = df_model, \n                 aes(x = trueSurv, \n                     y = after_stat(density),\n                     fill = group),\n                 binwidth = 1,\n                 alpha = .3,\n                 position = \"dodge\",\n                 inherit.aes = F) + \n  scale_fill_flat()\nplt_trueSurv_withHist\n\n\n\n\n\n\n\n\nFigure 3: Figure showing the true distribution with a histogram for the simulated dataset\n\n\n\n\n\nLooks good enough for our purposes. However, we haven’t exactly added in any censoring just now. So let’s do that, the censoring we’ll add in is called “right-censoring” as we won’t know when the event happened if it happened after the time we were observing for. This is the most common type, and is also the type that has the most work on as far as I can see. So we’ll keep it simple and grounded and just do right censoring.\nI’ll do this by randomly sampling between a small number and the true survival time if someone was censored. Each person will have a 50% chance of being censored so it should be roughly even. Though this might be quite a high rate of censoring. We’ll also be assuming that the censoring is uninformative and wouldn’t mean we couldn’t observe the event happening. Unfortunately, no one has found a cure for death, so I think we’re safe to make this assumption, but it is necessary to consider whether a competing event could occur that would stop a subject from being able to experience the event of interest.\n\n# TODO: Maybe chance the censoring... it makes sense that both groups should have the same chance of being censored at \n# all time points... currently it's based on survival time with one group haveing a lower survival time on average\ndf_model &lt;- df_model %&gt;% \n  mutate(\n    # censored = sample(c(0, 1), nrow(.), replace = T),\n    # censTime = runif(nrow(.), .01, trueSurv),\n    # obsTime = ifelse(censored, censTime, trueSurv)\n    censTime = runif(nrow(.), .01, .3 * max(trueSurv)),\n    censored = as.numeric(censTime &lt; trueSurv),\n    obsTime = ifelse(censored, censTime, trueSurv)\n  )\n\n\n\n\n\n\n\nNote\n\n\n\nKeen eyed individuals will notice that I have used 1 to show censoring and 0 to show the event of interest. I know this seems weird, but this is how the package brms expects the censoring information to be presented. I personally also like this, 1 means “this person was censored and so I only know they lived this long event free” and 0 means “this person was not censored and I know when the event happened”.\n\n\nEasy enough, just to confirm this worked, this is what our data look like now:\n\n\nCode\ndf_model %&gt;% \n  mutate(obsType = ifelse(censored, \"censored\", \"observed\"), \n         obsType = factor(obsType, levels= c(\"censored\", \"observed\"))) %&gt;% \n  ggplot(aes(obsTime, fill = group, alpha = obsType)) + \n  geom_histogram() + \n  scale_fill_flat() + \n  scale_alpha_manual(values = c(.4, 1)) + \n  facet_wrap(~group)\n\n\n\n\n\n\n\n\nFigure 4: Graph showing the censored survival times. People with a censored time (i.e., the event wasn’t observed) are in the more transparent column while the observed events are the solid colours\n\n\n\n\n\nSo, now we have a dataset with a bunch of censoring we can do some analysis and show why we don’t want to ignore censoring."
  },
  {
    "objectID": "blog/2025_03_12_LogisticOnCensoredData/index.html#ignoring-censorship",
    "href": "blog/2025_03_12_LogisticOnCensoredData/index.html#ignoring-censorship",
    "title": "The consequence of ignoring censorship",
    "section": "Ignoring censorship",
    "text": "Ignoring censorship\nFirst, I’ll need to set up the data to be amenable to what I think is the method employed often incorrectly. This method involves looking at who is still alive at some time point and comparing the two groups by way of a logistic regression. In this method, only people that live beyond the time point or who have died by this point are considered in the data. The problem with this is that we are removing a lot of people that could be still alive. We know that they were alive until a certain point, just not until the time point we’re interested in. However, this still gives us information about the likelihood of survival. For example, what if 90% of our population that didn’t die before the time point were known to be alive up until the day before? Hopefully we can recognise that it would be ridiculous to ignore them from our calculations for the probability of survival until time point \\(t\\)?\nSo, let’s make the data:\n\nTOI &lt;- 10\n\ndf_logi &lt;- df_model %&gt;% \n  filter(obsTime &gt; TOI | (!censored & obsTime &lt; TOI)) %&gt;% \n  mutate(logiSurv = obsTime &gt; TOI)\n\nThe code above simply keeps people that had an observed time that was greater than the Time of Interest (TOI) or those that had experienced the event before the TOI. We then say people were alive if they were in the study longer then the TOI. Sounds simple, but isn’t a great use of data. In this instance we have had to remove 27.25% of our dataset. We also have people that experienced the event almost immediately after the TOI, but we’re ignoring that for this analysis.\n\n\n\n\n\n\nNote\n\n\n\nJust wanted to highlight that I have many grievances with this approach, if that wasn’t immediately clear.\n\n\nNow we can fit a simple logistic regression to these data to get the “probability of survival at 5 years”\n\nm_log &lt;- brm(logiSurv ~ group, \n             family = \"bernoulli\", \n             data = df_logi, \n             chains = 1, \n             iter = 2000, \n             warmup = 1000)\n\nRunning \"C:/PROGRA~1/R/R-44~1.2/bin/x64/Rcmd.exe\" SHLIB foo.c\nusing C compiler: 'gcc.exe (GCC) 13.3.0'\ngcc  -I\"C:/PROGRA~1/R/R-44~1.2/include\" -DNDEBUG   -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/Rcpp/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/unsupported\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/BH/include\" -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/src/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppParallel/include/\" -DRCPP_PARALLEL_USE_TBB=1 -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include \"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp\"  -std=c++1y    -I\"C:/rtools44/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall  -mfpmath=sse -msse2 -mstackrealign  -c foo.c -o foo.o\ncc1.exe: warning: command-line option '-std=c++14' is valid for C++/ObjC++ but not for C\nIn file included from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/Core:19,\n                 from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/Dense:1,\n                 from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\nC:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [C:/PROGRA~1/R/R-44~1.2/etc/x64/Makeconf:289: foo.o] Error 1\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.134 seconds (Warm-up)\nChain 1:                0.107 seconds (Sampling)\nChain 1:                0.241 seconds (Total)\nChain 1: \n\n\nNow we’ve fit the model, let’s have a look at the output and see what we think\n\nsummary(m_log)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: logiSurv ~ group \n   Data: df_logi (Number of observations: 582) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.65      0.13    -0.91    -0.42 1.00      935      609\ngroupcase    -0.22      0.18    -0.58     0.15 1.00      817      577\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFrom this model output, we can see that the likelihood of a case being alive after 10 years is 29.71% [24.59% | 34.95%] compared to the controls who had a likelihood of 34.44% [28.76% | 39.71%]. This might sound reasonable, and reflects that the case group is 0.82 [0.53 | 1.10] as likely to have lived for the observation period. This is all well and good, but we know that this ignores a fair amount of information. So, let’s fit a more appropriate model and see what the estimates look like."
  },
  {
    "objectID": "blog/2025_03_12_LogisticOnCensoredData/index.html#accounting-for-censorship",
    "href": "blog/2025_03_12_LogisticOnCensoredData/index.html#accounting-for-censorship",
    "title": "The consequence of ignoring censorship",
    "section": "Accounting for censorship",
    "text": "Accounting for censorship\nWe’ll be fitting a Weibull distribution as we know that this is how the data were generated. In the real world we wouldn’t know this, so we would fit several models and look for different specifications, but that isn’t the point in this document. I’m simply trying to demonstrate the issue with ignoring censoring in these sorts of data.\nSo, let’s fit the model\n\nm_weib &lt;- brm(bf(obsTime | cens(censored) ~ group, \n                 shape ~ group), \n              family = \"weibull\", \n              data = df_model, \n              iter = 2000, \n              warmup = 1000, \n              chains = 1)\n\nRunning \"C:/PROGRA~1/R/R-44~1.2/bin/x64/Rcmd.exe\" SHLIB foo.c\nusing C compiler: 'gcc.exe (GCC) 13.3.0'\ngcc  -I\"C:/PROGRA~1/R/R-44~1.2/include\" -DNDEBUG   -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/Rcpp/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/unsupported\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/BH/include\" -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/src/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppParallel/include/\" -DRCPP_PARALLEL_USE_TBB=1 -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include \"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp\"  -std=c++1y    -I\"C:/rtools44/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall  -mfpmath=sse -msse2 -mstackrealign  -c foo.c -o foo.o\ncc1.exe: warning: command-line option '-std=c++14' is valid for C++/ObjC++ but not for C\nIn file included from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/Core:19,\n                 from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/Dense:1,\n                 from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\nC:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [C:/PROGRA~1/R/R-44~1.2/etc/x64/Makeconf:289: foo.o] Error 1\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000499 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.99 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.534 seconds (Warm-up)\nChain 1:                2.584 seconds (Sampling)\nChain 1:                5.118 seconds (Total)\nChain 1: \n\n\nNow we have our new model, we can look at some values in order to compare the estimates. Something that would be interesting first is to compare the model predictions to the real data. We can do this by plotting the estimates for the distribution of survival times over Figure 3 as this is the true distribution of survival times.\n\n\nCode\n# convert from brms parameterisation \nget_scale &lt;- function(mu, shape){\n  exp(mu)/gamma(1 + 1/exp(shape))\n}\n\n# get weib post\npost_weib &lt;- as_draws_df(m_weib) %&gt;% \n  select(contains(\"b_\")) %&gt;% \n  mutate(b_groupcase = b_groupcase + b_Intercept, \n         b_shape_groupcase = b_shape_Intercept + b_shape_groupcase) %&gt;% \n  rowid_to_column(var = \"iter\") %&gt;% \n  gather(contains(\"b_\"), \n         key = \"Param\", \n         value = \"value\") %&gt;% \n  mutate(paramType = ifelse(grepl(\"shape\", Param), \"shape\", \"mu\"), \n         group = ifelse(grepl(\"groupcase\", Param), \"case\", \"control\")) %&gt;% \n  select(-Param) %&gt;% \n  spread(paramType, value) %&gt;% \n  mutate(scale = get_scale(mu, shape)) \n\npltData_weib &lt;- post_weib %&gt;% \n  expand_grid(time = seq(.1, round(max(df_model$trueSurv)), .2)) %&gt;% \n  mutate(prob = dweibull(time, exp(shape), scale)) %&gt;% \n  group_by(group, time) %&gt;% \n  mean_hdci(prob)\n\nplt_trueSurv_withHist + \n  geom_ribbon(data = pltData_weib, \n              aes(y = prob, x = time, \n                  ymin = .lower, ymax = .upper, \n                  fill = group), \n              colour = \"transparent\", \n              alpha = .3) + \n  geom_line(data = pltData_weib, \n            aes(y = prob, x = time), \n            linetype = \"dashed\")\n\n\n\n\n\n\n\n\nFigure 5: Plot showing the true survival times as solid lines, with a histogram for the simulated data. Model predictions are shown with a dashed line for the mean estimate and a shaded region to show the 95% Credibility Intervals.\n\n\n\n\n\nHopefully we’re fairly convinced that this model has done a pretty good job of retrieving the true values that generated this data, we can now calculate the likelihood that someone would live until 10 years. This is fairly easily done, all we need to do is look a the cumulative density distribution for our specified model and see what the density is at 5 years.\n\n\nCode\nplt_cpdf_weib &lt;- post_weib %&gt;% \n  expand_grid(time = 0:round(max(df_model$trueSurv))) %&gt;% \n  mutate(prob = 1 - pweibull(time, exp(shape), scale)) %&gt;% \n  group_by(group, time) %&gt;% \n  mean_hdci(prob) %&gt;% \n  ggplot(aes(time, prob, colour = group, fill = group)) + \n  geom_path(linetype = \"longdash\") + \n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              alpha = .3, \n              colour = \"transparent\") + \n  scale_colour_flat() +\n  scale_fill_flat() \nplt_cpdf_weib\n\n\n\n\n\n\n\n\nFigure 6: This shows the inverse of the cumulative density for the weibull distribution using the posterior draws for the weibull model. This can be interpreted as the estimated likelihood of being alive at the time point on the x axis.\n\n\n\n\n\nFrom this, we can see that the model suggests that the likelihood of survival for cases at 10 years is 37.46% [32.71% | 41.91%] and for controls it is estimated to be 37.46% [32.71% | 41.91%]. This doesn’t appear to match up with the logistic model. The logistic model, although uncertain, suggested that our controls had a lower probability of experiencing the event by 10 years. So which is right?\nSince we made this data, we can know everything about it. I’ve waited until now to make this abundantly clear, but we are actually able to show the true survival probability at 10 years by simply calculating this from the initial parameters used to create this data. I’ll leave it as an exercise for you to compare the different approaches in terms of which did a better job of estimating this true value.\n\nwriteLines(paste0(\"control: \", round(exp(-(TOI / cont_scale) ^ cont_shape) * 100, 2), \"%\\n\",\n                  \"case:    \", round(exp(-(TOI / case_scale) ^ case_shape) * 100, 2), \"%\"))\n\ncontrol: 36.79%\ncase:    36.79%\n\n\nUnsurprisingly, our model that used the correct distribution makes a better estimate for the true values. So, why did this happen? There was some slight of hand earlier, though I’m sure plenty of people would notice it. When defining the parameters for the simulation, I deliberately picked two shape parameters that would cause the hazard to cross over at some point for the two groups. It isn’t hard to imagine circumstances in which this could happen; for instance, the exposed group (cases) could have a condition that comes with a high risk early on but if managed correctly can result in better outcomes for these people compared to a control group without adequate monitoring. The risk with using a logistic regression for this type of analysis is that the conclusions you make are biased by the time point selected, and the type of censoring present in the data. If we had modelled the survival at 5 years we may have had different conclusions as the ratio of the hazard was not constant over exposure time. In doing a logistic regression on these data, we remove nuance from our understanding of the data itself and could end up making inappropriate conclusions."
  },
  {
    "objectID": "blog/2025_03_12_LogisticOnCensoredData/index.html#a-different-time-point",
    "href": "blog/2025_03_12_LogisticOnCensoredData/index.html#a-different-time-point",
    "title": "The consequence of ignoring censorship",
    "section": "A different time point",
    "text": "A different time point\nIf we look at Figure 6, we can see that 10 is roughly the point at which the hazard functions cross over. So what happens if we look at a different time point, say maybe half this so we have 5 years observation instead? Annoyingly, we’ll have to process the data again for the logistic regression and run the model again in order to get some results, but for the Weibull model we just need to get estimates for a different time point. So, let’s re-run the logistic regression quick.\n\nTOI_2 &lt;- TOI/2\ndf_logi_2 &lt;- df_model %&gt;% \n  filter(obsTime &gt; TOI_2 | (!censored & obsTime &lt; TOI_2)) %&gt;% \n  mutate(logiSurv = obsTime &gt; TOI_2)\n\n\nm_log_2 &lt;- brm(logiSurv ~ group, \n               family = \"bernoulli\", \n               data = df_logi_2, \n               chains = 1, \n               iter = 2000, \n               warmup = 1000)\n\nRunning \"C:/PROGRA~1/R/R-44~1.2/bin/x64/Rcmd.exe\" SHLIB foo.c\nusing C compiler: 'gcc.exe (GCC) 13.3.0'\ngcc  -I\"C:/PROGRA~1/R/R-44~1.2/include\" -DNDEBUG   -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/Rcpp/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/unsupported\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/BH/include\" -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/src/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/\"  -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppParallel/include/\" -DRCPP_PARALLEL_USE_TBB=1 -I\"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include \"C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp\"  -std=c++1y    -I\"C:/rtools44/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall  -mfpmath=sse -msse2 -mstackrealign  -c foo.c -o foo.o\ncc1.exe: warning: command-line option '-std=c++14' is valid for C++/ObjC++ but not for C\nIn file included from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/Core:19,\n                 from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/Dense:1,\n                 from C:/Users/s01wj1/AppData/Local/R/win-library/4.4/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\nC:/Users/s01wj1/AppData/Local/R/win-library/4.4/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [C:/PROGRA~1/R/R-44~1.2/etc/x64/Makeconf:289: foo.o] Error 1\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.48 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.106 seconds (Warm-up)\nChain 1:                0.098 seconds (Sampling)\nChain 1:                0.204 seconds (Total)\nChain 1: \n\n\nNow we can simply run what we did before to get the comparisons for the probability of survival until 5 years for both models.\n\npost_m_log_2 &lt;- as_draws_df(m_log_2) %&gt;% \n  select(contains(\"b_\")) %&gt;% \n  rowid_to_column(var = \"iter\")\n\nprob_m_log_2 &lt;- post_m_log_2 %&gt;% \n  mutate(b_groupcase = b_Intercept + b_groupcase) %&gt;% \n  gather(contains(\"b_\"), \n         key = \"Param\", \n         value = \"logOdds\") %&gt;% \n  mutate(Param = str_remove(Param, \"b_\"), \n         prob = plogis(logOdds)) %&gt;% \n  group_by(Param) %&gt;% \n  mean_hdci(prob)\n\npaste(\"control:\", reportProb(prob_m_log_2, \"Intercept\"))\n\n[1] \"control: 74.04% [69.19% | 78.28%]\"\n\npaste(\"case:   \", reportProb(prob_m_log_2, \"groupcase\"))\n\n[1] \"case:    54.28% [48.94% | 59.40%]\"\n\n\nAnd now the Weibull distribution?\n\nprob_m_weib &lt;- plt_cpdf_weib[[\"data\"]] %&gt;% \n  filter(time == TOI_2)\n\npaste(\"control:\", reportProb(prob_m_weib, \"control\", \"group\"))\n\n[1] \"control: 76.70% [73.51% | 80.52%]\"\n\npaste(\"case:   \", reportProb(prob_m_weib, \"case\", \"group\"))\n\n[1] \"case:    57.52% [53.73% | 62.01%]\"\n\n\nAnd now the truth?\n\npaste0(\"control: \", round(exp(-(TOI_2 / cont_scale) ^ cont_shape) * 100, 2), \"%\")\n\n[1] \"control: 77.88%\"\n\npaste0(\"case:    \", round(exp(-(TOI_2 / case_scale) ^ case_shape) * 100, 2), \"%\")\n\n[1] \"case:    58.52%\"\n\n\nAt this time point, the logistic model isn’t performing too poorly; however, do we really fancy running a model at each time point to estimate the hazard ratio to ensure that we have a constant ratio? I wouldn’t think this would be the case. Further, we have also seen how the estimates for the likelihood of survival can be inaccurate. In the idea of transparency, I believe it is more robust to employ a survival analysis technique that can adequately account for censorship in the data as well as characterise the risk over time and detect non-proportional hazards.\nIn effect, from this analysis we can see that the choice of time point in the logistic regression can alter the conclusions drawn from the analysis. Having this fixed time point means we run the risk of making incorrect conclusions about the survival rates of different groups due to a almost arbitrary choice about when we decide is the most interesting time point. Having a model that can account for censorship appropriately and also examine survival times is a much more suitable approach.\nI can imagine one counter argument being that this approach is too heavy handed if we pick a time point that has very little to no censorship. Realistically, I guess this is a decent argument if you have a strong reason to believe that a particular time point is the most important aspect of the analysis. However, my counter question would be: do you really want to risk being wrong about the time point? And also, isn’t it nice to use a slightly more sophisticated approach that can offer more detailed insights? These are kind of rhetorical questions, but feel free to disagree with me."
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "Here’s a list of papers I have written or been involved in. This page will hopefully look prettier after I’ve had a think about how best to show this sort of information.\n\nJames, W. R., Reuther, J., Angus, E., Clarke, A. D., & Hunt, A. R. (2019). Inefficient eye movements: Gamification improves task execution, but not fixation strategy. Vision, 3(3), 48. doi\nClarke, A. D., Irons, J. L., James, W., Leber, A. B., & Hunt, A. R. (2022). Stable individual differences in strategies within, but not between, visual search tasks. Quarterly Journal of Experimental Psychology, 75(2), 289-296. doi\nJames, W., Hunt, A. R., & Clarke, A. D. (2023). Six of one, half dozen of the other: Suboptimal prioritizing for equal and unequal alternatives. Memory & Cognition, 51(2), 486-503. doi\nClaydon, J., James, W. R., Clarke, A. D., & Hunt, A. R. (2024). The role of framing, agency and uncertainty in a focus-divide dilemma. Memory & Cognition, 52(3), 574-594. doi\nHollick, R. J., James, W. R., Nicoll, A., Locock, L., Black, C., Dhaun, N., … & Basu, N. (2024). Identifying key health system components associated with improved outcomes to inform the re-configuration of services for adults with rare autoimmune rheumatic diseases: a mixed-methods study. The Lancet Rheumatology, 6(6), e361-e373. doi\nJames, W., Black, C., Basu, N., Little, M. A., & Hollick, R. (2024). Sociodemographic Factors Associated with Clinic Non-attendance and Unscheduled Emergency Care Episodes in ANCA-associated Vasculitis. ACR Convergence 2024, conference abstract. link"
  }
]