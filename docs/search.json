[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Warren James, PhD",
    "section": "",
    "text": "I am a researcher interested in using a whole host of statistical methods to answer all sorts of interesting (not always) questions. The work I do is all in R and so I thought to save me time I could put some of this on a website somewhere that other people might also find useful.\nCurrently, I’m employed as part of the Epidemiology group, and the Biostatistics and Health Data Science Group as a research fellow where I use Electronic Health Care records to answer questions of clinical interest"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Some things I wrote about things I find interesting",
    "section": "",
    "text": "P-values and Clinical Decisions\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\nWarren James\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "this would be a page where I talk about myself because I’m cool like that"
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#simulate-visit-data",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#simulate-visit-data",
    "title": "VOICES_keyServiceComponents",
    "section": "Simulate visit data",
    "text": "Simulate visit data\nNow all the values are in the dataset, we can produce count data for the number of visits each patient had during the time they were in the study.\n\nDemonstration\nTo show that this function behaves in the expected way, I have generated two datasets using the bespoke nbinomProcess function and the built in rnbinom function to generate two datasets using the same parameters. The exposure time will be kept constant to allow for an easier comparison to be made with the true distribution.\n\n# set some parameters\nN &lt;- 1e6\nthisMu &lt;- 3.4\nthisSize &lt;- 10.3\n\n# generate data using the rnbinom() function\ndf_rnbinom &lt;- tibble(subj = 1:N, \n                     nEvents = rnbinom(N, thisSize, mu = thisMu), \n                     thisFunction = \"rnbinom\")\n\n# generate data using the nbinomProcess() function\ndf_nbinomProcess &lt;- tibble(subj = 1:N,\n                           thisFunction = \"nbinomProcess\") \ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  nbinomProcess(thisMu, thisSize, 1)\n})\ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  length(df_nbinomProcess$nEvents[[i]])\n})\n\n# create figure to compare \nrbind(df_rnbinom, df_nbinomProcess) %&gt;% \n  ggplot(aes(nEvents, fill = thisFunction)) + \n  geom_histogram(binwidth = 1, \n                 position = \"dodge\") + \n  geom_line(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                          y = dnbinom(x, thisSize, mu = thisMu) * N),\n            aes(x, y), \n            inherit.aes = F) +\n  geom_point(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                           y = dnbinom(x, thisSize, mu = thisMu) * N),\n             aes(x, y), \n             size = 3,\n             inherit.aes = F) +\n  scale_fill_flat()\n\n\n\n\n\n\n\n\nFrom the above, we can see that the functions produce very similar results that both match onto the true distribution with a high enough number of samples. The reason for the nbinomProcess function is to produce a number of events with time stamps to make data that looks similar to what would be available for analysis."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#create-synthetic-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#create-synthetic-dataset",
    "title": "VOICES_keyServiceComponents",
    "section": "Create synthetic dataset",
    "text": "Create synthetic dataset\n\n# generate a series of positive and negative events \n# For the purposes of this dataset, I've made it so that everyone has 3 \"negative\" events\ntoBind &lt;- t(sapply(1:nrow(df_patientData), function(i){\n  time &lt;- df_patientData$timeYears[i]\n  genVisits(3, df_patientData$mu[i], df_patientData$shape[i] * time, time)\n})) \n\n# bind the data and unnest the list columns \ndf_patientData &lt;- df_patientData %&gt;% \n  cbind(toBind) %&gt;% \n  unnest(c(posEvent, EventTime))"
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#format-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#format-dataset",
    "title": "VOICES_keyServiceComponents",
    "section": "Format dataset",
    "text": "Format dataset\nNow we have the event times, we can make the dataset look a touch more realistic.\n\n# end Date of study to get index data for each patient\nendDate &lt;- as.Date(\"2020-10-31\") \n\n# function to convert between numbers and the data\nChangeTime &lt;- function(og_date, thisYears){\n  if(thisYears[1] &lt; 0){\n    sub &lt;- T\n    thisYears &lt;- abs(thisYears)\n  } else {\n    sub &lt;- F\n  }\n  \n  # sort year value\n  floorYears &lt;- floor(thisYears)\n\n  # sort month value\n  thisMonth &lt;- (thisYears - floorYears) * 12\n  floorMonth &lt;- floor(thisMonth)\n  \n  # sort day value\n  thisDay &lt;- thisMonth - floorMonth\n  thisDay &lt;- round(thisDay * 28)\n  \n  # have to do the months as days because it goes wonky with certain dates otherwise \n  if(sub){\n    og_date -(years(floorYears) + days(floorMonth * 28) + days(thisDay)) \n  } else {\n    og_date + (years(floorYears) + days(floorMonth * 28) + days(thisDay))   \n  }\n}\n\n# format the dataset to look more realistic\ndf_patientData &lt;- df_patientData %&gt;% \n  select(-c(Intercept, shape, iter, mu)) %&gt;%\n  mutate(across(c(AdviceandorLed:WaitTimeNewWithinWeek), ~ abs(.x) &gt; 0), \n         indexDate = ChangeTime(endDate, -timeYears), \n         eventDate = ChangeTime(indexDate, EventTime)) %&gt;% \n  select(subj, indexDate, Board, timeYears, everything(), -EventTime)\n\n# add in an mcon column which is based on whether the event was \"positive\"\n# NB: The \"Codes_X\" variables are defined in the markdown document which can be \n#     found in the github repository. \n#     They are the same as the codes seen in the readME file for the repository. \ndf_patientData$mcon &lt;- sapply(1:nrow(df_patientData), function(i){\n  if(df_patientData$posEvent[i]) {\n    sample(Codes_SI, 1)\n  } else {\n    sample(c(Codes_Cancer, Codes_CVD), 1)\n  }\n})\n\n# Add in some other codes\ndf_patientData$ocon &lt;- sapply(1:nrow(df_patientData), function(i){\n  nOthers &lt;- sample(0:5, 1)\n  if(df_patientData$posEvent[i]){\n    output &lt;- sample(c(Codes_SI, Codes_Cancer, Codes_CVD), nOthers)\n  } else {\n    output &lt;- sample(c(Codes_Cancer, Codes_CVD), nOthers)\n  }\n  paste(c(output, rep(\"NA\", 5 - nOthers)), collapse = \"|\")\n})\n\n# tidy up the dataset \ndf_patientData &lt;- df_patientData %&gt;% \n  separate(ocon, into = paste0(\"ocon\", c(1:5)), sep = \"[|]\")\n\n# show the dataset\nreactable(df_patientData)\n\n\n\n\n\nPlease note, the ICD-10 codes that don’t relate to Serious Infection were sample from two other lists regarding Cancer and Cardiovascular Disease so these probably aren’t the most clinically valid. The extra codes are simply there to give an impression of what the real data looks like."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#run-model",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#run-model",
    "title": "VOICES_keyServiceComponents",
    "section": "Run model",
    "text": "Run model\nThe model was fit using the brms library. All variables regarding the service components were entered as main effects. In the paper, additional parameters include Sex (deviation scaled), Age (median centered and decade scaled), Urban/Rural status (2 fold classification), and Scottish Index of Multiple Deprivation (as quintiles with 3 set as the baseline). Additionally, there was a random intercept for board of treatment to account for differences in base rates of Serious Infection. For this example, these haven’t been included. All coefficients regarding the service components (and the demographic features in the paper) were given a weakly informative prior of a student t distribution with a mean of 0, degrees of freedom of 3, and standard deviation of 1. A comparison between the prior predictions and posterior predictions can be seen by fitting the same model with the sample_prior argument set to \"only\".\n\nmy_prior &lt;- prior(student_t(3, 0, 1), class = \"b\")\n\n# A model that only samples the prior \n# I've commented this out because it doesn't need to be run here \n# but should you want to check the prior assumptions, this is the code you need\n# m_nbinomn_priorOnly &lt;- brm(\n#   SI_events | rate(timeYears) ~\n#     AdviceandorLed + CohortedClinic + JointParaClinic + \n#     LocalAAVpath + OwnDayCaseUnit + VascMDT + \n#     WaitTimeNewWithinWeek, \n#   data = df_modelData, \n#   family = negbinomial(), \n#   prior = my_prior,\n#   sample_prior = \"only\",\n#   chains = 1,\n#   iter = 4000, \n#   warmup = 3000,\n#   control = list(adapt_delta = .9)\n# )\n\nm_nbinom &lt;- brm(\n  SI_events | rate(timeYears) ~\n    AdviceandorLed + CohortedClinic + JointParaClinic + \n    LocalAAVpath + OwnDayCaseUnit + VascMDT + \n    WaitTimeNewWithinWeek, \n  data = df_modelData, \n  family = negbinomial(), \n  prior = my_prior, \n  chains = 1,\n  iter = 4000, \n  warmup = 2000,\n  control = list(adapt_delta = .9)\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000848 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 8.48 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 1: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 7.768 seconds (Warm-up)\nChain 1:                7.393 seconds (Sampling)\nChain 1:                15.161 seconds (Total)\nChain 1: \n\nsummary(m_nbinom)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: SI_events | rate(timeYears) ~ AdviceandorLed + CohortedClinic + JointParaClinic + LocalAAVpath + OwnDayCaseUnit + VascMDT + WaitTimeNewWithinWeek \n   Data: df_modelData (Number of observations: 850) \n  Draws: 1 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    -1.66      0.15    -1.95    -1.37 1.00     1750\nAdviceandorLedTRUE            0.10      0.18    -0.25     0.45 1.00     1436\nCohortedClinicTRUE           -0.15      0.09    -0.32     0.02 1.00     1600\nJointParaClinicTRUE           0.05      0.09    -0.14     0.21 1.00     2229\nLocalAAVpathTRUE             -0.13      0.10    -0.32     0.06 1.00     1728\nOwnDayCaseUnitTRUE            0.11      0.11    -0.10     0.32 1.00     1581\nVascMDTTRUE                  -0.14      0.17    -0.47     0.21 1.00     1562\nWaitTimeNewWithinWeekTRUE    -0.04      0.12    -0.29     0.21 1.00     1851\n                          Tail_ESS\nIntercept                     1593\nAdviceandorLedTRUE            1297\nCohortedClinicTRUE            1514\nJointParaClinicTRUE           1451\nLocalAAVpathTRUE              1405\nOwnDayCaseUnitTRUE            1630\nVascMDTTRUE                   1090\nWaitTimeNewWithinWeekTRUE     1606\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     0.16      0.02     0.13     0.20 1.00     2058     1515\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAbove, the model results can be show in terms of the coefficient values. The first thing to notice is that the Credibility Intervals are more narrow than those of in the paper, but this is primarily due to there being less noise in the estimates associated with patients having varying demographic features and no noise added due to random effects. In essence, this simulated data only allows access to service components to vary with all other features being the same. Generally, the results look very similar to those in the paper despite the limitations."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#show-model-results",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#show-model-results",
    "title": "VOICES_keyServiceComponents",
    "section": "Show model results",
    "text": "Show model results\nSimply looking at the summary isn’t always the easiest way to understand these results, so below I’ve created a few plots to demonstrate make things clearer. The first thing to do is get the posterior draws.\n\npost_nbinom &lt;- m_nbinom %&gt;% \n  as.matrix() %&gt;% \n  as_tibble() %&gt;% \n  select(-c(lp__, lprior)) \n\n\nPlotting results\n\nRaw coefficients\nThis figure shows the raw coefficients on a log scale. With the expection of the Intercept, all values can be compared to the 0 line in order to get a sense for the direction of the effect.\n\n\n\n\n\n\n\n\n\n\n\nIncident Rate Ratios\nThis figure shows the Incident Rate Ratio for each service component. These values correspond to the ratio of the rate between a service component being present and not being present. Values greater than one show an increase in the rate, while values lower than one show a decrease in the rate.\n\n\n\n\n\n\n\n\n\n\n\nResults as rates\nThis figure shows the results as rate values. This way compares how the presence of different service components was associated with changes in the yearly rate of Serious Infection. The vertical line represents the mean Intercept rate with the shaded region showing the 95% credibility interval. Please note, this doesn’t show the range of predicted counts, but the uncertainty in the estimate for the mean parameter (i.e., the mean rate of events in these data)."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#expected-number-of-events-over-time",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/identifyingKeyComponents.html#expected-number-of-events-over-time",
    "title": "VOICES_keyServiceComponents",
    "section": "Expected number of events over time",
    "text": "Expected number of events over time\nThis plot shows the total number of expected events over time in the absence of any of the service components (in grey). Each facet shows how this number of events could be expected to change this count compared to having no other service components. The larger shaded region shows where 95% of the data would be expected to lie in a negative binomial distribution using the point estimates from the model outputs. The solid line shows the mean number of events to be expected given the parameter values. The shaded region between the dashed lines shows the 95% credibility interval around the estimate for the mean value.\n\n# poorly named function... please don't do this, make useful function names\n# this function is also really specific to this problem, so it would need to be adapted \n# for any other model/dataset\n# Using the rnbinom() function is susceptible to the upper and lower bounds moving about due to \n# the nature of these data being integers... \nmakePlot &lt;- function(thisModel, minTime, maxTime, res = .02) {\n  post &lt;- thisModel %&gt;% as.matrix() %&gt;% \n    as_tibble() %&gt;% \n    select(-c(lprior, lp__)) %&gt;% \n    mutate(across(2:(ncol(.) - 1), ~ .x + b_Intercept),\n           across(1:(ncol(.) - 1), ~ exp(.x))) %&gt;% \n    gather(1:(ncol(.)), \n           key = \"param\", \n           value = \"coef\") %&gt;% \n    group_by(param) %&gt;% \n    mean_hdci(coef)  %&gt;% \n    expand_grid(time = seq(minTime, maxTime, res)) %&gt;% \n    group_by(param) %&gt;% \n    mutate(iter = row_number()) %&gt;% \n    select(iter, everything()) %&gt;% \n    ungroup() \n  \n  quantile_data &lt;- post %&gt;% \n    select(param, coef, time) %&gt;% \n    spread(param, coef) %&gt;% \n    gather(2:(ncol(.) - 1), \n           key = \"param\", \n           value = \"mu\") %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5 = qnbinom(.5, shape * time, mu = mu * time),\n           q.975 = qnbinom(.975, shape * time, mu = mu * time),\n           q.025 = qnbinom(.025, shape * time, mu = mu * time))\n  \n  mu_post &lt;- post %&gt;% \n    select(-c(.width, .point, .interval)) %&gt;%  \n    filter(!param %in% c(\"shape\", \"b_Intercept\")) %&gt;% \n    left_join(post %&gt;%\n                filter(param == \"shape\") %&gt;% \n                select(iter, coef) %&gt;% \n                rename(shape = coef)) %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5_med = qnbinom(.5, shape * time, mu = coef * time),\n           q.5_upr = qnbinom(.5, shape * time, mu = .upper * time),\n           q.5_lwr = qnbinom(.5, shape * time, mu = .lower * time)) \n  \n  quantile_data[quantile_data$param != \"Intercept\",] %&gt;% \n    ggplot(aes(time, q.5, colour = param)) + \n    geom_path(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n              aes(time, q.5),\n              colour = \"black\") +\n    geom_ribbon(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n                aes(ymin = q.025, ymax = q.975), \n                fill = \"black\", \n                colour = \"transparent\",\n                alpha = .1) + \n    geom_path() + \n    geom_ribbon(aes(ymin = q.025, ymax = q.975, \n                    fill = param), \n                colour = \"transparent\",\n                alpha = .2) +\n    geom_ribbon(data = mu_post,\n                aes(x = time,\n                    ymin = q.5_lwr - .2, ymax = q.5_upr + .2,\n                    y = q.5_med,\n                    fill = param),\n                alpha = .2,\n                linetype = \"longdash\") +\n    scale_x_continuous(\"Time since Index Date\") + \n    scale_y_continuous(\"Estimated total number of Serious Infections\") +\n    facet_wrap(~param) +\n    theme(legend.position = \"bottom\", \n          panel.grid.major.y = element_line())\n  \n  # return(post)\n}\n\nmakePlot(m_nbinom, .1, 24) \n\nJoining with `by = join_by(iter)`"
  },
  {
    "objectID": "supMat/index.html",
    "href": "supMat/index.html",
    "title": "Supplementary Materials",
    "section": "",
    "text": "VOICES: Identidying Key Service Components\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nWarren James\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#simulate-visit-data",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#simulate-visit-data",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Simulate visit data",
    "text": "Simulate visit data\nNow all the values are in the dataset, we can produce count data for the number of visits each patient had during the time they were in the study.\n\nDemonstration\nTo show that this function behaves in the expected way, I have generated two datasets using the bespoke nbinomProcess function and the built in rnbinom function to generate two datasets using the same parameters. The exposure time will be kept constant to allow for an easier comparison to be made with the true distribution.\n\n# set some parameters\nN &lt;- 1e6\nthisMu &lt;- 3.4\nthisSize &lt;- 10.3\n\n# generate data using the rnbinom() function\ndf_rnbinom &lt;- tibble(subj = 1:N, \n                     nEvents = rnbinom(N, thisSize, mu = thisMu), \n                     thisFunction = \"rnbinom\")\n\n# generate data using the nbinomProcess() function\ndf_nbinomProcess &lt;- tibble(subj = 1:N,\n                           thisFunction = \"nbinomProcess\") \ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  nbinomProcess(thisMu, thisSize, 1)\n})\ndf_nbinomProcess$nEvents &lt;- sapply(1:N, function(i){\n  length(df_nbinomProcess$nEvents[[i]])\n})\n\n# create figure to compare \nrbind(df_rnbinom, df_nbinomProcess) %&gt;% \n  ggplot(aes(nEvents, fill = thisFunction)) + \n  geom_histogram(binwidth = 1, \n                 position = \"dodge\") + \n  geom_line(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                          y = dnbinom(x, thisSize, mu = thisMu) * N),\n            aes(x, y), \n            inherit.aes = F) +\n  geom_point(data = tibble(x = 0:max(c(df_rnbinom$nEvents, df_nbinomProcess$nEvents)), \n                           y = dnbinom(x, thisSize, mu = thisMu) * N),\n             aes(x, y), \n             size = 3,\n             inherit.aes = F) +\n  scale_fill_flat()\n\n\n\n\n\n\n\n\nFrom the above, we can see that the functions produce very similar results that both match onto the true distribution with a high enough number of samples. The reason for the nbinomProcess function is to produce a number of events with time stamps to make data that looks similar to what would be available for analysis."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#create-synthetic-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#create-synthetic-dataset",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Create synthetic dataset",
    "text": "Create synthetic dataset\n\n# generate a series of positive and negative events \n# For the purposes of this dataset, I've made it so that everyone has 3 \"negative\" events\ntoBind &lt;- t(sapply(1:nrow(df_patientData), function(i){\n  time &lt;- df_patientData$timeYears[i]\n  genVisits(3, df_patientData$mu[i], df_patientData$shape[i] * time, time)\n})) \n\n# bind the data and unnest the list columns \ndf_patientData &lt;- df_patientData %&gt;% \n  cbind(toBind) %&gt;% \n  unnest(c(posEvent, EventTime))"
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#format-dataset",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#format-dataset",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Format dataset",
    "text": "Format dataset\nNow we have the event times, we can make the dataset look a touch more realistic.\n\n# end Date of study to get index data for each patient\nendDate &lt;- as.Date(\"2020-10-31\") \n\n# function to convert between numbers and the data\nChangeTime &lt;- function(og_date, thisYears){\n  if(thisYears[1] &lt; 0){\n    sub &lt;- T\n    thisYears &lt;- abs(thisYears)\n  } else {\n    sub &lt;- F\n  }\n  \n  # sort year value\n  floorYears &lt;- floor(thisYears)\n\n  # sort month value\n  thisMonth &lt;- (thisYears - floorYears) * 12\n  floorMonth &lt;- floor(thisMonth)\n  \n  # sort day value\n  thisDay &lt;- thisMonth - floorMonth\n  thisDay &lt;- round(thisDay * 28)\n  \n  # have to do the months as days because it goes wonky with certain dates otherwise \n  if(sub){\n    og_date -(years(floorYears) + days(floorMonth * 28) + days(thisDay)) \n  } else {\n    og_date + (years(floorYears) + days(floorMonth * 28) + days(thisDay))   \n  }\n}\n\n# format the dataset to look more realistic\ndf_patientData &lt;- df_patientData %&gt;% \n  select(-c(Intercept, shape, iter, mu)) %&gt;%\n  mutate(across(c(AdviceandorLed:WaitTimeNewWithinWeek), ~ abs(.x) &gt; 0), \n         indexDate = ChangeTime(endDate, -timeYears), \n         eventDate = ChangeTime(indexDate, EventTime)) %&gt;% \n  select(subj, indexDate, Board, timeYears, everything(), -EventTime)\n\n# add in an mcon column which is based on whether the event was \"positive\"\n# NB: The \"Codes_X\" variables are defined in the markdown document which can be \n#     found in the github repository. \n#     They are the same as the codes seen in the readME file for the repository. \ndf_patientData$mcon &lt;- sapply(1:nrow(df_patientData), function(i){\n  if(df_patientData$posEvent[i]) {\n    sample(Codes_SI, 1)\n  } else {\n    sample(c(Codes_Cancer, Codes_CVD), 1)\n  }\n})\n\n# Add in some other codes\ndf_patientData$ocon &lt;- sapply(1:nrow(df_patientData), function(i){\n  nOthers &lt;- sample(0:5, 1)\n  if(df_patientData$posEvent[i]){\n    output &lt;- sample(c(Codes_SI, Codes_Cancer, Codes_CVD), nOthers)\n  } else {\n    output &lt;- sample(c(Codes_Cancer, Codes_CVD), nOthers)\n  }\n  paste(c(output, rep(\"NA\", 5 - nOthers)), collapse = \"|\")\n})\n\n# tidy up the dataset \ndf_patientData &lt;- df_patientData %&gt;% \n  separate(ocon, into = paste0(\"ocon\", c(1:5)), sep = \"[|]\")\n\n# show the dataset\nreactable(df_patientData)\n\n\n\n\n\nPlease note, the ICD-10 codes that don’t relate to Serious Infection were sample from two other lists regarding Cancer and Cardiovascular Disease so these probably aren’t the most clinically valid. The extra codes are simply there to give an impression of what the real data looks like."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#run-model",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#run-model",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Run model",
    "text": "Run model\nThe model was fit using the brms library. All variables regarding the service components were entered as main effects. In the paper, additional parameters include Sex (deviation scaled), Age (median centered and decade scaled), Urban/Rural status (2 fold classification), and Scottish Index of Multiple Deprivation (as quintiles with 3 set as the baseline). Additionally, there was a random intercept for board of treatment to account for differences in base rates of Serious Infection. For this example, these haven’t been included. All coefficients regarding the service components (and the demographic features in the paper) were given a weakly informative prior of a student t distribution with a mean of 0, degrees of freedom of 3, and standard deviation of 1. A comparison between the prior predictions and posterior predictions can be seen by fitting the same model with the sample_prior argument set to \"only\".\n\nmy_prior &lt;- prior(student_t(3, 0, 1), class = \"b\")\n\n# A model that only samples the prior \n# I've commented this out because it doesn't need to be run here \n# but should you want to check the prior assumptions, this is the code you need\n# m_nbinomn_priorOnly &lt;- brm(\n#   SI_events | rate(timeYears) ~\n#     AdviceandorLed + CohortedClinic + JointParaClinic + \n#     LocalAAVpath + OwnDayCaseUnit + VascMDT + \n#     WaitTimeNewWithinWeek, \n#   data = df_modelData, \n#   family = negbinomial(), \n#   prior = my_prior,\n#   sample_prior = \"only\",\n#   chains = 1,\n#   iter = 4000, \n#   warmup = 3000,\n#   control = list(adapt_delta = .9)\n# )\n\nm_nbinom &lt;- brm(\n  SI_events | rate(timeYears) ~\n    AdviceandorLed + CohortedClinic + JointParaClinic + \n    LocalAAVpath + OwnDayCaseUnit + VascMDT + \n    WaitTimeNewWithinWeek, \n  data = df_modelData, \n  family = negbinomial(), \n  prior = my_prior, \n  chains = 1,\n  iter = 4000, \n  warmup = 2000,\n  control = list(adapt_delta = .9)\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.00048 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.8 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 4000 [  0%]  (Warmup)\nChain 1: Iteration:  400 / 4000 [ 10%]  (Warmup)\nChain 1: Iteration:  800 / 4000 [ 20%]  (Warmup)\nChain 1: Iteration: 1200 / 4000 [ 30%]  (Warmup)\nChain 1: Iteration: 1600 / 4000 [ 40%]  (Warmup)\nChain 1: Iteration: 2000 / 4000 [ 50%]  (Warmup)\nChain 1: Iteration: 2001 / 4000 [ 50%]  (Sampling)\nChain 1: Iteration: 2400 / 4000 [ 60%]  (Sampling)\nChain 1: Iteration: 2800 / 4000 [ 70%]  (Sampling)\nChain 1: Iteration: 3200 / 4000 [ 80%]  (Sampling)\nChain 1: Iteration: 3600 / 4000 [ 90%]  (Sampling)\nChain 1: Iteration: 4000 / 4000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 8.195 seconds (Warm-up)\nChain 1:                19.736 seconds (Sampling)\nChain 1:                27.931 seconds (Total)\nChain 1: \n\nsummary(m_nbinom)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: SI_events | rate(timeYears) ~ AdviceandorLed + CohortedClinic + JointParaClinic + LocalAAVpath + OwnDayCaseUnit + VascMDT + WaitTimeNewWithinWeek \n   Data: df_modelData (Number of observations: 850) \n  Draws: 1 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 2000\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                    -1.66      0.15    -1.95    -1.37 1.00     1750\nAdviceandorLedTRUE            0.10      0.18    -0.25     0.45 1.00     1436\nCohortedClinicTRUE           -0.15      0.09    -0.32     0.02 1.00     1600\nJointParaClinicTRUE           0.05      0.09    -0.14     0.21 1.00     2229\nLocalAAVpathTRUE             -0.13      0.10    -0.32     0.06 1.00     1728\nOwnDayCaseUnitTRUE            0.11      0.11    -0.10     0.32 1.00     1581\nVascMDTTRUE                  -0.14      0.17    -0.47     0.21 1.00     1562\nWaitTimeNewWithinWeekTRUE    -0.04      0.12    -0.29     0.21 1.00     1851\n                          Tail_ESS\nIntercept                     1593\nAdviceandorLedTRUE            1297\nCohortedClinicTRUE            1514\nJointParaClinicTRUE           1451\nLocalAAVpathTRUE              1405\nOwnDayCaseUnitTRUE            1630\nVascMDTTRUE                   1090\nWaitTimeNewWithinWeekTRUE     1606\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     0.16      0.02     0.13     0.20 1.00     2058     1515\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAbove, the model results can be show in terms of the coefficient values. The first thing to notice is that the Credibility Intervals are more narrow than those of in the paper, but this is primarily due to there being less noise in the estimates associated with patients having varying demographic features and no noise added due to random effects. In essence, this simulated data only allows access to service components to vary with all other features being the same. Generally, the results look very similar to those in the paper despite the limitations."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#show-model-results",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#show-model-results",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Show model results",
    "text": "Show model results\nSimply looking at the summary isn’t always the easiest way to understand these results, so below I’ve created a few plots to demonstrate make things clearer. The first thing to do is get the posterior draws.\n\npost_nbinom &lt;- m_nbinom %&gt;% \n  as.matrix() %&gt;% \n  as_tibble() %&gt;% \n  select(-c(lp__, lprior)) \n\n\nPlotting results\n\nRaw coefficients\nThis figure shows the raw coefficients on a log scale. With the expection of the Intercept, all values can be compared to the 0 line in order to get a sense for the direction of the effect.\n\n\n\n\n\n\n\n\n\n\n\nIncident Rate Ratios\nThis figure shows the Incident Rate Ratio for each service component. These values correspond to the ratio of the rate between a service component being present and not being present. Values greater than one show an increase in the rate, while values lower than one show a decrease in the rate.\n\n\n\n\n\n\n\n\n\n\n\nResults as rates\nThis figure shows the results as rate values. This way compares how the presence of different service components was associated with changes in the yearly rate of Serious Infection. The vertical line represents the mean Intercept rate with the shaded region showing the 95% credibility interval. Please note, this doesn’t show the range of predicted counts, but the uncertainty in the estimate for the mean parameter (i.e., the mean rate of events in these data)."
  },
  {
    "objectID": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#expected-number-of-events-over-time",
    "href": "supMat/2024_06_IdentifyingKeySystemComponents/index.html#expected-number-of-events-over-time",
    "title": "VOICES: Identidying Key Service Components",
    "section": "Expected number of events over time",
    "text": "Expected number of events over time\nThis plot shows the total number of expected events over time in the absence of any of the service components (in grey). Each facet shows how this number of events could be expected to change this count compared to having no other service components. The larger shaded region shows where 95% of the data would be expected to lie in a negative binomial distribution using the point estimates from the model outputs. The solid line shows the mean number of events to be expected given the parameter values. The shaded region between the dashed lines shows the 95% credibility interval around the estimate for the mean value.\n\n# poorly named function... please don't do this, make useful function names\n# this function is also really specific to this problem, so it would need to be adapted \n# for any other model/dataset\n# Using the rnbinom() function is susceptible to the upper and lower bounds moving about due to \n# the nature of these data being integers... \nmakePlot &lt;- function(thisModel, minTime, maxTime, res = .02) {\n  post &lt;- thisModel %&gt;% as.matrix() %&gt;% \n    as_tibble() %&gt;% \n    select(-c(lprior, lp__)) %&gt;% \n    mutate(across(2:(ncol(.) - 1), ~ .x + b_Intercept),\n           across(1:(ncol(.) - 1), ~ exp(.x))) %&gt;% \n    gather(1:(ncol(.)), \n           key = \"param\", \n           value = \"coef\") %&gt;% \n    group_by(param) %&gt;% \n    mean_hdci(coef)  %&gt;% \n    expand_grid(time = seq(minTime, maxTime, res)) %&gt;% \n    group_by(param) %&gt;% \n    mutate(iter = row_number()) %&gt;% \n    select(iter, everything()) %&gt;% \n    ungroup() \n  \n  quantile_data &lt;- post %&gt;% \n    select(param, coef, time) %&gt;% \n    spread(param, coef) %&gt;% \n    gather(2:(ncol(.) - 1), \n           key = \"param\", \n           value = \"mu\") %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5 = qnbinom(.5, shape * time, mu = mu * time),\n           q.975 = qnbinom(.975, shape * time, mu = mu * time),\n           q.025 = qnbinom(.025, shape * time, mu = mu * time))\n  \n  mu_post &lt;- post %&gt;% \n    select(-c(.width, .point, .interval)) %&gt;%  \n    filter(!param %in% c(\"shape\", \"b_Intercept\")) %&gt;% \n    left_join(post %&gt;%\n                filter(param == \"shape\") %&gt;% \n                select(iter, coef) %&gt;% \n                rename(shape = coef)) %&gt;% \n    mutate(param = str_remove(param, \"b_\"), \n           param = str_remove(param, \"TRUE\"),\n           q.5_med = qnbinom(.5, shape * time, mu = coef * time),\n           q.5_upr = qnbinom(.5, shape * time, mu = .upper * time),\n           q.5_lwr = qnbinom(.5, shape * time, mu = .lower * time)) \n  \n  quantile_data[quantile_data$param != \"Intercept\",] %&gt;% \n    ggplot(aes(time, q.5, colour = param)) + \n    geom_path(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n              aes(time, q.5),\n              colour = \"black\") +\n    geom_ribbon(data = quantile_data[quantile_data$param == \"Intercept\",] %&gt;% \n                select(-param), \n                aes(ymin = q.025, ymax = q.975), \n                fill = \"black\", \n                colour = \"transparent\",\n                alpha = .1) + \n    geom_path() + \n    geom_ribbon(aes(ymin = q.025, ymax = q.975, \n                    fill = param), \n                colour = \"transparent\",\n                alpha = .2) +\n    geom_ribbon(data = mu_post,\n                aes(x = time,\n                    ymin = q.5_lwr - .2, ymax = q.5_upr + .2,\n                    y = q.5_med,\n                    fill = param),\n                alpha = .2,\n                linetype = \"longdash\") +\n    scale_x_continuous(\"Time since Index Date\") + \n    scale_y_continuous(\"Estimated total number of Serious Infections\") +\n    facet_wrap(~param) +\n    theme(legend.position = \"bottom\", \n          panel.grid.major.y = element_line())\n  \n  # return(post)\n}\n\nmakePlot(m_nbinom, .1, 24) \n\nJoining with `by = join_by(iter)`"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html",
    "title": "P-values and Clinical Decisions",
    "section": "",
    "text": "I was asked by an old master’s student for some advice about presenting the results of some work they had done looking at readmission amongst people discharged to a ward vs. being sent home. Obviously, we can appreciate that there may be some differences in terms of the outcomes between people sent home and those remaining in hospital. While I was very excited to see him using R for his analysis and Rmarkdown to show his working, something else didn’t sit well with me. His question was whether there was any “significant” difference in the probability of these people being readmitted to hospital 30 days after being discharged.\nI have no qualms with discussing the problems of significance testing in a host of different contexts (follow this link for a fantastic discussion of inappropriate applications of significance testing), and as such this grabbed my attention. The approach I prefer to take is one that looks at what we could reasonably surmise given the data that is in front of us. Therefore, I prefer to employ Bayesian methods to answer most (scientific) questions I’m presented with.\nBeing the pedant I am, I took the work and offered an alternative perspective that I believe safeguarded against some of the issues that are mentioned in the paper I referenced above. The aim of this isn’t to discourage people from using p-values, but to try and anticipate how your research would be used by stakeholders. If they aren’t likely to understand the nuances of Null Hypothesis Significance Testing, or may misinterpret the results, then maybe it’s not the best idea to use these tools.\n\n\nHere are the packages I used for this post:\n\nlibrary(tidyverse) # data processing\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidybayes) # for tidy format model draws\nlibrary(brms)      # Bayesian regression\n\nLoading required package: Rcpp\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\nAttaching package: 'brms'\n\nThe following objects are masked from 'package:tidybayes':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(see)       # prettier colour schemes\nlibrary(ggridges)  # a nice package for nice plots\n\n\nAttaching package: 'ggridges'\n\nThe following objects are masked from 'package:tidybayes':\n\n    scale_point_color_continuous, scale_point_color_discrete,\n    scale_point_colour_continuous, scale_point_colour_discrete,\n    scale_point_fill_continuous, scale_point_fill_discrete,\n    scale_point_size_continuous\n\n# set the theme \ntheme_set(theme_bw() + \n            theme(legend.position = \"bottom\"))\n\n# for reproducibility \nset.seed(970431)\n\n# The colours I used for any plots \nmyColours &lt;- c(\"#c62828\", \"#283593\", \"#2e7d32\", \"#f9a825\")"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#packages-used",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#packages-used",
    "title": "P-values and Clinical Decisions",
    "section": "",
    "text": "Here are the packages I used for this post:\n\nlibrary(tidyverse) # data processing\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidybayes) # for tidy format model draws\nlibrary(brms)      # Bayesian regression\n\nLoading required package: Rcpp\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\nAttaching package: 'brms'\n\nThe following objects are masked from 'package:tidybayes':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(see)       # prettier colour schemes\nlibrary(ggridges)  # a nice package for nice plots\n\n\nAttaching package: 'ggridges'\n\nThe following objects are masked from 'package:tidybayes':\n\n    scale_point_color_continuous, scale_point_color_discrete,\n    scale_point_colour_continuous, scale_point_colour_discrete,\n    scale_point_fill_continuous, scale_point_fill_discrete,\n    scale_point_size_continuous\n\n# set the theme \ntheme_set(theme_bw() + \n            theme(legend.position = \"bottom\"))\n\n# for reproducibility \nset.seed(970431)\n\n# The colours I used for any plots \nmyColours &lt;- c(\"#c62828\", \"#283593\", \"#2e7d32\", \"#f9a825\")"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty",
    "title": "P-value and Clinical Decisions",
    "section": "visualising the uncertainty",
    "text": "visualising the uncertainty\nFirst, I considered that the dataset is very small. This raised the question of: what sort of probability values could reasonably produce data like these? Making the assumption that a beta distribution could suitably represent these data, I produced a plot to perhaps answer this question.\n\nx &lt;- seq(0, .3, length.out = 100)\ndf_beta &lt;- df_fisher %&gt;% \n  expand_grid(tibble(x = x)) %&gt;% \n  mutate(dens = dbeta(x, totalReadmit, totalNoReadmit)) \n\ndf_beta %&gt;% \n  ggplot(aes(x, dens, colour = Group)) + \n  geom_path() \n\n\n\n\n\n\n\n\nWe can see that there’s a large degree of overlap in these two distributions. However a quick visual inspection would suggest that the Ward group was more likely to not be readmitted given the majority of the curve is lower than the Home group. Given we have such small numbers, it is unsurprising we didn’t observe a significant difference.\nTherefore, I felt it best to offer a Bayesian approach. The way I viewed these data is that for 51 people, we have a record or whether they were readmitted after being discharged, and a grouping variable of where they were discharged to. So this seemed like a straightforward logistic regression problem"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#model-fitting",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#model-fitting",
    "title": "P-values and Clinical Decisions",
    "section": "Model fitting",
    "text": "Model fitting\nFirst thing’s first, I fit a model using the default priors from the brms package.\n\nm_defaultPriors &lt;- brm(readmit ~ Group, \n                       data = df_readmission, \n                       chains = 1,\n                       family = \"bernoulli\", \n                       sample_prior = T, \n                       seed = 987421)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.028 seconds (Warm-up)\nChain 1:                0.028 seconds (Sampling)\nChain 1:                0.056 seconds (Total)\nChain 1: \n\n# show model summary \nsummary(m_defaultPriors)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: readmit ~ Group \n   Data: df_readmission (Number of observations: 51) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -3.35      0.99    -5.55    -1.81 1.01      427      433\nGroupHome     0.79      1.29    -1.68     3.40 1.00      403      489\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLike most people, I’m not able to transform these values onto a scale that makes sense off the top of my head. So, I’ve made a quick function that will summarise the the outputs on a ratio scale instead. Note, there’s almost definitely a neater function out there, but we’re not here to make the prettiest most efficient functions.\n\n# quick function as we'll be doing this again \nmyOddsRatio &lt;- function(thisModel){\n  as_draws_df(thisModel) %&gt;% \n    select(b_GroupHome) %&gt;% \n    mean_hdci(b_GroupHome, .width = c(.95)) %&gt;% \n    mutate(across(1:3, ~exp(.x)))\n}\n\nmyOddsRatio(m_defaultPriors) \n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 1 × 6\n  b_GroupHome .lower .upper .width .point .interval\n        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1        2.21  0.166   26.2   0.95 mean   hdci     \n\n\nFrom the above output, we can see there is still a very large amount of uncertainty about this estimate. However, the Bayesian model is a bit more certain than the original Fisher test. For those that need more convincing, take a look at Figure 2 and you can clearly see that the bayesian model offers a more precise estimate.\n\n\nCode\n# setup dataset for use later\ndf_comparison &lt;- tibble(\n  testType = c(\"Fisher\",\n               \"Bayes_defaultPriors\"), \n  mu       = c(fisherResults$estimate,\n               exp(fixef(m_defaultPriors)[2, 1])), \n  .lower   = c(fisherResults$conf.int[1], \n               exp(fixef(m_defaultPriors)[2, 3])), \n  .upper   = c(fisherResults$conf.int[2], \n               exp(fixef(m_defaultPriors)[2, 4]))\n) %&gt;% \n  mutate(testType = factor(testType, levels = c(\"Fisher\", \"Bayes_defaultPriors\")))\n\ndf_comparison %&gt;% \n  ggplot(aes(mu, testType, colour = testType)) + \n  geom_vline(xintercept = 1) +\n  geom_point() + \n  geom_linerange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_log10(\"Odds Ratio\") + \n  scale_y_discrete(\"\") + \n  scale_colour_manual(values = myColours)\n\n\n\n\n\n\n\n\nFigure 2: Odds Ratio for the likelihood of readmission to hospital when being discharged ‘home’ compared to a ‘ward’. The x-axis is on a log10 scaled for clarity.\n\n\n\n\n\nAlthough the Bayesian model estimates are less uncertain, we still have a large degree of uncertainty in our estimate. While this may simply be a result of having a small dataset, there are other things we should also consider when fitting these models. Given we’re using Bayesian stats now, we have the luxury of adding prior beliefs to our model. While priors can be subjective, we have a very real reason to not rely exclusively on default (or, even worse, uniform) priors when dealing with logistic regression."
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty-in-the-data",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#visualising-the-uncertainty-in-the-data",
    "title": "P-values and Clinical Decisions",
    "section": "visualising the uncertainty in the data",
    "text": "visualising the uncertainty in the data\nFirst, I considered that the dataset is very small. This raised the question of: what sort of probability values could reasonably produce data like these? Making the assumption that a beta distribution could suitably represent these data, I produced a plot to perhaps answer this question.\n\nx &lt;- seq(0, .3, length.out = 100)\ndf_beta &lt;- df_fisher %&gt;% \n  expand_grid(tibble(x = x)) %&gt;% \n  mutate(dens = dbeta(x, totalReadmit, totalNoReadmit)) \n\ndf_beta %&gt;% \n  ggplot(aes(x, dens, colour = Group)) + \n  geom_path() + \n  scale_colour_manual(values = myColours)\n\n\n\n\n\n\n\nFigure 1: This figure shows the likelihood of various values that could have produced the observed data.\n\n\n\n\n\nWe can see that there’s a large degree of overlap in these two distributions. However a quick visual inspection would suggest that the Ward group was more likely to not be readmitted given the majority of the curve is lower than the Home group. Given we have such small numbers, it is unsurprising we didn’t observe a significant difference.\nTherefore, I felt it best to offer a Bayesian approach. The way I viewed these data is that for 51 people, we have a record or whether they were readmitted after being discharged, and a grouping variable of where they were discharged to. So this seemed like a straightforward logistic regression problem"
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#more-informative-priors",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#more-informative-priors",
    "title": "P-values and Clinical Decisions",
    "section": "More informative priors",
    "text": "More informative priors\n\nInitial beliefs\nWe can retrieve the default priors from the model we implemented by using the handy function get_prior.\n\nget_prior(readmit ~ Group, \n          data = df_readmission,\n          family = \"bernoulli\")\n\n                prior     class      coef group resp dpar nlpar lb ub\n               (flat)         b                                      \n               (flat)         b GroupHome                            \n student_t(3, 0, 2.5) Intercept                                      \n       source\n      default\n (vectorized)\n      default\n\n\nIn this case, we have flat priors on the coefficient for the effect of being discharged home. In effect, this means that every value from \\(-\\infty\\) to \\(\\infty\\). Which doesn’t sound very reasonable to me. However, this becomes a particular issue when handling models looking at probability. When we use the inverse logistic function on this belief (to put the values onto a probability scale), we see something undesirable happen.\n\n# get random numbers from some range \nx &lt;- runif(1e5, -5, 5)\n\n# perform the inverse logit function\ninv_x &lt;- plogis(x)\n\n# plot this\ntibble(p = inv_x) %&gt;% \n  ggplot(aes(p)) + \n  geom_histogram(aes(y = ..density..))\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 3: Inverse logit transformation of randomly sampled numbers between -5 and 5.\n\n\n\n\n\nFor this example, we only used numbers between -5 and 5, which occupies an incredibly small region of the range \\(-\\infty\\) to \\(\\infty\\), and we can already see why using priors such as these may cause issues. What the figure shows is that there is much stronger initial belief in values that are at either extreme, and so this will lead to a poorer reflection of what can actually be seen in the data. If we were to use a different prior, we could perform the same steps to look at what the prior would look like. For example, what happens if we use a Student T distribution using the same values for the intercept term in the model we already ran?\n\nx_student &lt;- rstudent_t(n = 1e5, df = 3, mu = 0, sigma = 2.5)\n\ntibble(x_student = plogis(x_student)) %&gt;% \n  ggplot(aes(x_student)) + \n  geom_histogram(aes(y = ..density..))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 4: Inverse logit transformation of randomly sampled numbers from a Student t distribution with the parameters at df = 3, mu = 0, sigma = 2.5.\n\n\n\n\n\nLooks a little bit better, but what if we made the standard deviation smaller?\n\nx_student &lt;- rstudent_t(n = 1e5, df = 3, mu = 0, sigma = 1)\n\ntibble(x_student = plogis(x_student)) %&gt;% \n  ggplot(aes(x_student)) + \n  geom_histogram(aes(y = ..density..))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nFigure 5: Inverse logit transformation of randomly sampled numbers from a Student t distribution with the parameters at df = 3, mu = 0, sigma = 1.\n\n\n\n\n\nThis looks like a much more reasonable initial assumption; not only are the extremes represented to a lesser extent, there is also a smaller difference in the density between the lowest and highest points. Playing around with these values is encouraged to see how the prior belief changes, but hopefully this quick demonstration gives some insight into why this may cause a problem.\n\n\nrefitting the model\nNow we have an understanding of our model’s prior beliefs, and what these would mean for the results, we can use this to inform in a model and see how the results change.\n\n# add in somewhat informative priors \nm_studentPrior_sigma1 &lt;- update(m_defaultPriors, \n                                prior = c(prior(student_t(3, 0, 1), class = \"b\"), \n                                          prior(student_t(3, 0, 1), class = \"intercept\")))\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.027 seconds (Warm-up)\nChain 1:                0.028 seconds (Sampling)\nChain 1:                0.055 seconds (Total)\nChain 1: \n\nsummary(m_studentPrior_sigma1)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: readmit ~ Group \n   Data: df_readmission (Number of observations: 51) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -3.06      0.78    -4.77    -1.73 1.01      480      413\nGroupHome     0.39      0.84    -1.17     2.17 1.00      605      568\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we can add these results to our plot from before and see what this has done to the uncertainty in our estimates:\n\n\n\n\n\n\n\n\nFigure 6: Figure with the student t priors added to both the intercept and home coefficient.\n\n\n\n\n\nFrom Figure 6, we can now see that the uncertainty in our estimate is reduced by quite a substantial amount. Something else that might be beneficial for a model fit is to change the “mean” value in the prior. At the moment, we’ve centred the prior on a value of 50% which would seem to be a lot higher than the actual data suggests. In this case, we might try something on the lower end to compare the model results. For example, maybe we think there is a 10% change people would be readmitted on average.\n\n# rerun the model with the intercept as 10% \nm_studentPrior_10percent &lt;- update(m_defaultPriors, \n                                   prior = c(prior(student_t(3, 0, 1), class = \"b\"), \n                                             prior(student_t(3, qlogis(.1), 1), class = \"intercept\")))\n\nThe desired updates require recompiling the model\n\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 1:                0.037 seconds (Sampling)\nChain 1:                0.07 seconds (Total)\nChain 1: \n\nsummary(m_studentPrior_10percent)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: readmit ~ Group \n   Data: df_readmission (Number of observations: 51) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -3.07      0.81    -4.79    -1.72 1.00      527      551\nGroupHome     0.43      0.86    -1.10     2.23 1.00      720      415\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\ndf_comparison &lt;- df_comparison %&gt;% \n  bind_rows(\n    tibble(\n      testType = c(\"Bayes_studentT_10percent\"), \n      mu       = c(exp(fixef(m_studentPrior_10percent)[2, 1])), \n      .lower   = c(exp(fixef(m_studentPrior_10percent)[2, 3])), \n      .upper   = c(exp(fixef(m_studentPrior_10percent)[2, 4]))\n    )\n  ) %&gt;% \n  mutate(testType = factor(testType, levels = c(\"Fisher\", \"Bayes_defaultPriors\", \n                                                \"Bayes_studentT_sigma1\", \n                                                \"Bayes_studentT_10percent\")))\n\ndf_comparison %&gt;% \n  ggplot(aes(mu, testType, colour = testType)) + \n  geom_vline(xintercept = 1) +\n  geom_point() + \n  geom_linerange(aes(xmin = .lower, xmax = .upper)) +\n  scale_x_log10(\"Odds Ratio\") + \n  scale_y_discrete(\"\") + \n  scale_colour_manual(values = myColours)\n\n\n\n\n\n\n\nFigure 7: Figure with the student t priors added to both the intercept and home coefficient. The prior for the intercept now has a mean of 10%.\n\n\n\n\n\nHere, we don’t see much of a change in the Ratio, but there is a small gain in terms of certainty. Though, perhaps we’re not showing the model results in a way that is best suited to answering this question."
  },
  {
    "objectID": "blog/2024_11_06_ProblemsWithPValues/index.html#another-way-to-show-the-data",
    "href": "blog/2024_11_06_ProblemsWithPValues/index.html#another-way-to-show-the-data",
    "title": "P-values and Clinical Decisions",
    "section": "Another way to show the data",
    "text": "Another way to show the data\nAdmittedly, I have somewhat omitted a sin with the above figures, but I do have an explanation. Bayesian statistics isn’t pre-occupied with what IS and ISN’T significant, but rather more interested in what conclusions could reasonably be supported by the data. In showing only the 95% intervals, we’re hiding some of the information that the model is communicating to us. The reason I did this is because we have a mixture of Frequentist and Bayesian methods being applied and shown in the one figure, and I wanted to draw comparisons to the initial analysis.\nHowever, I think this pushes us towards the binary thinking that is often encourage by things like p-values and as such I think these figures should be avoided if possible. Additionally, if we circle back to the link I shared earlier which discusses a few examples of p-values being used incorrectly, figures like these run the risk of encouraging errors in judging what the results show. The risk we run is that the end user will look at these plots and wrongly infer: “The interval contains 1 (no effect), and so there is no difference between the groups, and therefore we should send everyone home”. This isn’t what the analysis shows, however it’s not impossible to imagine people mistakenly making this judgement. The way I see it, it is our job to try and avoid people making these incorrect statements and the best method for doing this is to clearly communicate the model results in a way that is more readily understood. Thankfully, Credibility Intervals make more intuitive sense, so we can use this to our advantage when plotting model results.\nIn this case, I opted to avoid the use of odds ratios and instead plot the predicted probability that someone would be readmitted given they were discharged home vs. to a ward.\n\n\nCode\n# function to get posterior summaries for these data\n# again, not very pretty\ngetPost &lt;- \\(listModels){\n  output &lt;- tibble()\n  for(i in 1:length(listModels)){\n    output &lt;- bind_rows(output, \n                        as_draws_df(listModels[[i]]) %&gt;% \n                          select(1:2) %&gt;% \n                          mutate(modelName = names(listModels)[i]) %&gt;% \n                          rowid_to_column(var = \"iter\"))\n  }\n  \n  return(output %&gt;% \n           mutate(b_GroupHome = b_Intercept + b_GroupHome, \n                  across(c(2:3), ~exp(.x))) %&gt;% \n           gather(2:3, \n                  key = \"param\", \n                  value = \"coef\") %&gt;% \n           mutate(dischargeTo = ifelse(param == \"b_Intercept\", \"Ward\", \"Home\")))\n}\n\ngetPost(list(default = m_defaultPriors, \n             studentT = m_studentPrior_sigma1, \n             tenPercent = m_studentPrior_10percent)) %&gt;% \n  ggplot(aes(dischargeTo, coef, colour = dischargeTo, fill = dischargeTo)) + \n  stat_pointinterval(point_interval = \"median_hdci\", \n                     .width = c(.5, .95), \n                     show.legend = F) + \n  stat_slabinterval(alpha = .3) +\n  # add in a crude estimate from the raw data\n  geom_point(data = df_readmission %&gt;% \n               rename(dischargeTo = Group) %&gt;% \n               group_by(dischargeTo) %&gt;% \n               summarise(N = n(), \n                         n = sum(readmit)) %&gt;% \n               mutate(perc = n/N), \n             aes(y = perc), \n             size = 5, pch = 1, \n             show.legend = F) +\n  facet_wrap(~modelName) + \n  scale_colour_manual(values = myColours) + \n  scale_fill_manual(values = myColours) + \n  scale_y_continuous(labels = scales::percent_format())\n\n\n\n\n\n\n\n\nFigure 8: Figure showing the entire posterior for the Bayesian models. The solid point shows the median posterior estimate with the 50% (thick) and 95% (thin) credibility intervals as lines. The hollow points show the crude estimate calculated directly from the data.\n\n\n\n\n\nThe benefit of figure 8 is that we can now see the range of values that would be supported by these data. Although we can’t clearly estimate the magnitude of difference, what we can see is that the data support higher probabilities of readmissions for people discharged home than to a ward. Although this isn’t always the case, we can see that there is some reason to believe these people may be at more risk, though we would want more data in order to be even more certain. By coupling this figure with the previous figures looking at the odds Ratios, we can get a more complete picture of what is happening. The odds ratios suggest an increase (albeit uncertain) in the probability of being readmitted when discharged home, while the entire posterior shows that the risk of readmission is relatively low in both groups. In having the entire posterior visible, we can more readily make intuitive judgements about what the model is showing without having to rely on measures like p-values.\nMy thoughts are that the p-value in and of itself isn’t a very useful number to take away. It doesn’t really help us to understand what is happening in the data, but instead is a somewhat simplified “answer” to what are usually complicated questions. In showing the uncertainty in our estimates (either the ratio, or the probability of readmission), we can see that the data are more supportive of there being a small different between the outcomes. However, the most clear take away here is that we could use more data to inform our estimates."
  }
]